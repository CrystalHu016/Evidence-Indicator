#!/usr/bin/env python3
"""
ä¼˜åŒ–ç‰ˆé«˜çº§RAGç³»ç»Ÿ - å‡å°‘APIè°ƒç”¨ï¼Œæå‡æ€§èƒ½
"""

import os
import re
import time
import numpy as np
from typing import List, Tuple, Dict, Optional, Any
from dataclasses import dataclass
from pydantic import SecretStr
import openai
from langchain_community.document_loaders import JSONLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.schema import Document
from langchain_openai import OpenAIEmbeddings
from langchain_chroma import Chroma
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


@dataclass
class EvidenceRange:
    """æ ¹æ‹ æƒ…å ±ã®æ–‡å­—åˆ—ç¯„å›²"""
    start_char: int
    end_char: int
    evidence_text: str
    source_document: str


@dataclass
class RAGResult:
    """RAGæ¤œç´¢çµæœ"""
    answer: str
    hit_chunks: List[str]
    source_document: str
    evidence_range: EvidenceRange
    strategy_used: str
    confidence_score: float


class FastChunkEvaluator:
    """é«˜é€Ÿãƒãƒ£ãƒ³ã‚¯è©•ä¾¡å™¨ï¼ˆAPIå‘¼å‡ºã—å‰Šæ¸›ç‰ˆï¼‰"""
    
    def __init__(self):
        self.vectorizer = TfidfVectorizer(stop_words='english')
        
    def can_answer_question_fast(self, chunk: str, question: str) -> Tuple[bool, float]:
        """
        TF-IDFãƒ™ãƒ¼ã‚¹ã®é«˜é€Ÿè©•ä¾¡
        """
        try:
            # TF-IDFé¡ä¼¼åº¦ã‚’è¨ˆç®—
            documents = [question, chunk]
            tfidf_matrix = self.vectorizer.fit_transform(documents)
            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
            
            # åŸºæœ¬çš„ãªåˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯
            if similarity > 0.3:
                return True, similarity
            else:
                return False, similarity
                
        except Exception as e:
            print(f"é«˜é€Ÿè©•ä¾¡ã‚¨ãƒ©ãƒ¼: {e}")
            return False, 0.0


class OptimizedRAGSystem:
    """æœ€é©åŒ–ã•ã‚ŒãŸRAGã‚·ã‚¹ãƒ†ãƒ """
    
    # ã‚¯ãƒ©ã‚¹å¤‰æ•°ã§ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥
    _instance = None
    _initialized = False
    
    def __new__(cls, openai_api_key: str, chroma_path: str, data_path: str):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance
    
    def __init__(self, openai_api_key: str, chroma_path: str, data_path: str):
        if self._initialized:
            return
            
        self.openai_api_key = openai_api_key
        self.chroma_path = chroma_path
        self.data_path = data_path
        self.embedding_function = OpenAIEmbeddings(api_key=SecretStr(openai_api_key))
        self.fast_evaluator = FastChunkEvaluator()
        
        print("ğŸš€ åˆæœŸåŒ–ä¸­...")
        
        # Chroma DBæ¥ç¶š
        self.db = Chroma(persist_directory=chroma_path, embedding_function=self.embedding_function)
        
        # æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰
        self.documents = self._load_documents()
        self.document_map = self._create_document_map()
        
        print("âœ… åˆæœŸåŒ–å®Œäº†")
        self._initialized = True
    
    def _load_documents(self) -> List[Document]:
        """æ–‡æ›¸ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿"""
        try:
            loader = JSONLoader(
                file_path=self.data_path,
                jq_schema='.[ ]',
                content_key='output'
            )
            return loader.load()
        except Exception as e:
            print(f"æ–‡æ›¸èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return []
    
    def _create_document_map(self) -> Dict[str, Document]:
        """æ–‡æ›¸å†…å®¹ã‹ã‚‰æ–‡æ›¸ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã¸ã®ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½œæˆ"""
        doc_map = {}
        for doc in self.documents:
            # æ–‡æ›¸ã®æœ€åˆã®100æ–‡å­—ã‚’ã‚­ãƒ¼ã¨ã—ã¦ä½¿ç”¨
            key = doc.page_content[:100]
            doc_map[key] = doc
        return doc_map
    
    def strategy_1_optimized(self, query: str) -> Optional[RAGResult]:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸå¯¾å¿œç­–ï¼‘ï¼šå®Œå…¨æ–‡æ›¸ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
        """
        # é€šå¸¸ã®æ¤œç´¢
        search_results = self.db.similarity_search_with_relevance_scores(query, k=1)
        
        if not search_results:
            return None
        
        # æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„ãƒãƒ£ãƒ³ã‚¯ã‚’é¸æŠ
        hit_chunk = search_results[0][0]
        hit_score = search_results[0][1]
        
        # ãƒãƒ£ãƒ³ã‚¯ã®å…ƒæ–‡æ›¸ã‚’åŠ¹ç‡çš„ã«ç‰¹å®š
        source_doc = self._find_source_document_fast(hit_chunk.page_content)
        if not source_doc:
            return None
        
        full_document_text = source_doc.page_content
        
        # ç°¡æ˜“çš„ãªæ ¹æ‹ ç¯„å›²æŠ½å‡ºï¼ˆAPIå‘¼å‡ºã—å‰Šæ¸›ï¼‰
        evidence_range = self._extract_evidence_range_simple(full_document_text, query)
        
        # å›ç­”ç”Ÿæˆï¼ˆ1å›ã®APIå‘¼å‡ºã—ã®ã¿ï¼‰
        answer = self._generate_answer(full_document_text, query)
        
        return RAGResult(
            answer=answer,
            hit_chunks=[hit_chunk.page_content],
            source_document=full_document_text,
            evidence_range=evidence_range,
            strategy_used="æœ€é©åŒ–å®Œå…¨æ–‡æ›¸ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ",
            confidence_score=hit_score
        )
    
    def strategy_2_optimized(self, query: str) -> Optional[RAGResult]:
        """
        æœ€é©åŒ–ã•ã‚ŒãŸå¯¾å¿œç­–ï¼’ï¼šé«˜é€Ÿé©å¿œãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
        """
        print("=== æœ€é©åŒ–å¯¾å¿œç­–ï¼’ï¼šé«˜é€Ÿé©å¿œãƒãƒ£ãƒ³ã‚­ãƒ³ã‚° ===")
        
        # æœ€åˆã«é–¢é€£æ–‡æ›¸ã‚’ç‰¹å®š
        search_results = self.db.similarity_search_with_relevance_scores(query, k=1)
        if not search_results:
            return None
        
        hit_chunk = search_results[0][0]
        source_doc = self._find_source_document_fast(hit_chunk.page_content)
        if not source_doc:
            return None
        
        document_text = source_doc.page_content
        
        # ã‚ˆã‚ŠåŠ¹ç‡çš„ãªãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚ºé¸æŠ
        chunk_sizes = [1000, 500]  # æ®µéšæ•°ã‚’å‰Šæ¸›
        
        best_result = None
        
        for step, chunk_size in enumerate(chunk_sizes, 1):
            print(f"STEP {step}: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º {chunk_size}")
            
            # å‹•çš„ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°ï¼ˆå˜ä¸€æ–‡æ›¸ã®ã¿ï¼‰
            chunks = self._create_chunks_from_document(document_text, chunk_size)
            if not chunks:
                continue
            
            # æœ€è‰¯ãƒãƒ£ãƒ³ã‚¯ã‚’é¸æŠ
            best_chunk = self._select_best_chunk_fast(chunks, query)
            if not best_chunk:
                continue
            
            # é«˜é€Ÿè©•ä¾¡
            can_answer, confidence = self.fast_evaluator.can_answer_question_fast(
                best_chunk['content'], query
            )
            
            print(f"å›ç­”å¯èƒ½æ€§: {can_answer}, ä¿¡é ¼åº¦: {confidence:.3f}")
            
            if can_answer and confidence > 0.4:
                # ååˆ†ãªä¿¡é ¼åº¦ã®å ´åˆã€çµæœã‚’ä¿å­˜
                best_result = best_chunk
                best_result['confidence'] = confidence
                best_result['chunk_size'] = chunk_size
                
                # æ¬¡ã®ãƒ¬ãƒ™ãƒ«ã‚‚è©¦ã™
                continue
            else:
                # ä¿¡é ¼åº¦ãŒä½ã„å ´åˆã€å‰ã®çµæœãŒã‚ã‚Œã°ä½¿ç”¨
                break
        
        if not best_result:
            return None
        
        # å›ç­”ç”Ÿæˆ
        answer = self._generate_answer(best_result['content'], query)
        evidence_range = self._extract_evidence_range_simple(best_result['content'], query)
        
        return RAGResult(
            answer=answer,
            hit_chunks=[best_result['content']],
            source_document=document_text,
            evidence_range=evidence_range,
            strategy_used=f"æœ€é©åŒ–é©å¿œãƒãƒ£ãƒ³ã‚­ãƒ³ã‚° (ã‚µã‚¤ã‚º: {best_result['chunk_size']})",
            confidence_score=best_result['confidence']
        )
    
    def _find_source_document_fast(self, chunk_content: str) -> Optional[Document]:
        """é«˜é€Ÿæ–‡æ›¸æ¤œç´¢"""
        # ã¾ãšå®Œå…¨ä¸€è‡´ã‚’è©¦ã™
        for doc in self.documents:
            if chunk_content in doc.page_content:
                return doc
        
        # éƒ¨åˆ†ä¸€è‡´ã‚‚è©¦ã™
        chunk_start = chunk_content[:50]
        for doc in self.documents:
            if chunk_start in doc.page_content:
                return doc
        
        return None
    
    def _create_chunks_from_document(self, document: str, chunk_size: int) -> List[Dict[str, Any]]:
        """å˜ä¸€æ–‡æ›¸ã‹ã‚‰ãƒãƒ£ãƒ³ã‚¯ã‚’ä½œæˆ"""
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_size // 10,  # 10%ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ©ãƒƒãƒ—
            length_function=len,
        )
        
        # å˜ä¸€æ–‡æ›¸ã‚’ãƒãƒ£ãƒ³ã‚­ãƒ³ã‚°
        doc_obj = Document(page_content=document)
        chunks = text_splitter.split_documents([doc_obj])
        
        result = []
        for chunk in chunks:
            result.append({
                'content': chunk.page_content,
                'metadata': chunk.metadata
            })
        
        return result
    
    def _select_best_chunk_fast(self, chunks: List[Dict[str, Any]], query: str) -> Optional[Dict[str, Any]]:
        """TF-IDFãƒ™ãƒ¼ã‚¹ã®é«˜é€Ÿãƒãƒ£ãƒ³ã‚¯é¸æŠ"""
        if not chunks:
            return None
        
        try:
            # å…¨ãƒãƒ£ãƒ³ã‚¯ã¨ã‚¯ã‚¨ãƒªã®TF-IDF
            texts = [query] + [chunk['content'] for chunk in chunks]
            vectorizer = TfidfVectorizer(stop_words='english')
            tfidf_matrix = vectorizer.fit_transform(texts)
            
            # ã‚¯ã‚¨ãƒªã¨å„ãƒãƒ£ãƒ³ã‚¯ã®é¡ä¼¼åº¦
            similarities = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:])
            
            # æœ€é«˜ã‚¹ã‚³ã‚¢ã®ãƒãƒ£ãƒ³ã‚¯ã‚’é¸æŠ
            best_idx = np.argmax(similarities[0])
            best_chunk = chunks[best_idx].copy()
            best_chunk['similarity'] = similarities[0][best_idx]
            
            return best_chunk
            
        except Exception as e:
            print(f"ãƒãƒ£ãƒ³ã‚¯é¸æŠã‚¨ãƒ©ãƒ¼: {e}")
            return chunks[0] if chunks else None
    
    def _extract_evidence_range_simple(self, document: str, query: str) -> EvidenceRange:
        """ç°¡æ˜“çš„ãªæ ¹æ‹ ç¯„å›²æŠ½å‡ºï¼ˆAPIå‘¼å‡ºã—ãªã—ï¼‰"""
        # ã‚¯ã‚¨ãƒªã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        query_words = set(re.findall(r'\w+', query.lower()))
        
        # æ–‡æ›¸ã‚’æ–‡ã«åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ.!?]', document)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        if not sentences:
            return EvidenceRange(0, min(100, len(document)), document[:100], document)
        
        # æœ€ã‚‚é–¢é€£æ€§ã®é«˜ã„æ–‡ã‚’é¸æŠ
        best_sentence = ""
        best_score = 0
        best_start = 0
        
        current_pos = 0
        for sentence in sentences:
            sentence_words = set(re.findall(r'\w+', sentence.lower()))
            overlap = len(query_words & sentence_words)
            
            if overlap > best_score:
                best_score = overlap
                best_sentence = sentence
                best_start = document.find(sentence, current_pos)
            
            current_pos = document.find(sentence, current_pos) + len(sentence)
        
        if best_sentence and best_start >= 0:
            return EvidenceRange(
                start_char=best_start,
                end_char=best_start + len(best_sentence),
                evidence_text=best_sentence,
                source_document=document
            )
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        evidence_text = document[:min(100, len(document))]
        return EvidenceRange(0, len(evidence_text), evidence_text, document)
    
    def _generate_answer(self, context: str, query: str) -> str:
        """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å›ç­”ã‚’ç”Ÿæˆ"""
        # è¨€èªæ¤œå‡º
        def is_japanese(text):
            japanese_chars = sum(1 for char in text if '\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FAF')
            return japanese_chars > len(text) * 0.1
        
        if is_japanese(query):
            system_prompt = "ã‚ãªãŸã¯æ—¥æœ¬èªã§æ­£ç¢ºã«å›ç­”ã™ã‚‹çŸ¥çš„ãªã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚è³ªå•ã«æ—¥æœ¬èªã§ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚"
            user_prompt = f"""
            ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {context}
            è³ªå•: {query}
            
            å›ç­”:
            """
        else:
            system_prompt = "You are an intelligent assistant that provides accurate and concise answers in English."
            user_prompt = f"""
            Context: {context}
            Question: {query}
            
            Answer:
            """
        
        try:
            response = openai.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_prompt}
                ],
                max_tokens=300
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            print(f"å›ç­”ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return "å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    
    def query(self, query_text: str) -> Optional[RAGResult]:
        """
        ãƒ¡ã‚¤ãƒ³ã‚¯ã‚¨ãƒªå‡¦ç†ï¼ˆè¶…é«˜é€Ÿç‰ˆï¼‰
        """
        # å¯¾å¿œç­–1ã®ã¿ã‚’ä½¿ç”¨ï¼ˆæœ€ã‚‚é«˜é€Ÿï¼‰
        result = self.strategy_1_optimized(query_text)
        
        if result and result.confidence_score > 0.5:
            return result
        
        # ä½ä¿¡é ¼åº¦ã®å ´åˆã®ã¿å¯¾å¿œç­–2ã‚’è©¦è¡Œ
        if not result or result.confidence_score < 0.3:
            result_2 = self.strategy_2_optimized(query_text)
            if result_2 and result_2.confidence_score > (result.confidence_score if result else 0):
                return result_2
        
        return result
    
    def format_output(self, result: RAGResult) -> str:
        """çµæœã‚’æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å‡ºåŠ›"""
        if not result:
            return "å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        output = f"""
ã€å›ç­”ã€‘
{result.answer}

ã€æ¤œç´¢ãƒ’ãƒƒãƒˆã®ãƒãƒ£ãƒ³ã‚¯ã‚’å«ã‚€æ–‡æ›¸ã€‘
{result.source_document}

ã€æ ¹æ‹ æƒ…å ±ã®æ–‡å­—åˆ—ç¯„å›²ã€‘
{result.evidence_range.start_char + 1}æ–‡å­—ç›®ã€œ{result.evidence_range.end_char}æ–‡å­—ç›®

ã€æ ¹æ‹ æƒ…å ±ã€‘
{result.evidence_range.evidence_text}
        """
        
        return output.strip()


def test_optimized_rag():
    """æœ€é©åŒ–RAGã‚·ã‚¹ãƒ†ãƒ ã®ãƒ†ã‚¹ãƒˆ"""
    import os
    from dotenv import load_dotenv
    
    load_dotenv()
    api_key = os.environ.get("OPENAI_API_KEY")
    
    if not api_key:
        print("âŒ OPENAI_API_KEY æœªè¨­å®š")
        return
    
    # ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–
    rag_system = OptimizedRAGSystem(
        openai_api_key=api_key,
        chroma_path="chroma",
        data_path="./data/test_sample.json"
    )
    
    # ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª
    test_queries = [
        "ã‚³ãƒ³ãƒã‚¤ãƒ³ã¨ã¯ä½•ã§ã™ã‹",
        "éŸ³ä½è»¢å€’ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ãã ã•ã„"
    ]
    
    for query in test_queries:
        print(f"\n{'='*50}")
        print(f"ãƒ†ã‚¹ãƒˆã‚¯ã‚¨ãƒª: {query}")
        print(f"{'='*50}")
        
        start_time = time.time()
        result = rag_system.query(query)
        total_time = time.time() - start_time
        
        if result:
            print(rag_system.format_output(result))
            print(f"\nã€å‡¦ç†æ™‚é–“ã€‘{total_time:.2f}ç§’")
        else:
            print("âŒ å›ç­”ã‚’ç”Ÿæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")


if __name__ == "__main__":
    test_optimized_rag()