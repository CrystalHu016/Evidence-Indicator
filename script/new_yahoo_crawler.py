#!/usr/bin/env python3
"""
New Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ Crawler
æ–°ã—ã„Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼
"""

import requests
from bs4 import BeautifulSoup
import json
import os
from datetime import datetime
from typing import List, Dict

# Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸»è¦åˆ†ç±»URL
category_urls = {
    'å›½å†…': 'https://news.yahoo.co.jp/categories/domestic',
    'å›½éš›': 'https://news.yahoo.co.jp/categories/world',
    'çµŒæ¸ˆ': 'https://news.yahoo.co.jp/categories/business',
    'ã‚¨ãƒ³ã‚¿ãƒ¡': 'https://news.yahoo.co.jp/categories/entertainment',
    'ã‚¹ãƒãƒ¼ãƒ„': 'https://news.yahoo.co.jp/categories/sports',
    'ITãƒ»ç§‘å­¦': 'https://news.yahoo.co.jp/categories/it',
    'ãƒ©ã‚¤ãƒ•': 'https://news.yahoo.co.jp/categories/life',
    'åœ°åŸŸ': 'https://news.yahoo.co.jp/categories/local',
}

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36'
}

def fetch_top_news(category_name: str, url: str, top_n: int = 2) -> List[Dict]:
    """
    Fetch top news from a specific category
    ç‰¹å®šã®ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ãƒˆãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
    """
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        # æ ¹æ®é¡µé¢ç»“æ„é€‰å–æ–°é—»æ¡ç›®ï¼ˆSelectoréœ€æ ¹æ®çœŸå®é¡µé¢è°ƒæ•´ï¼‰
        # ãƒšãƒ¼ã‚¸æ§‹é€ ã«åŸºã¥ã„ã¦ãƒ‹ãƒ¥ãƒ¼ã‚¹é …ç›®ã‚’é¸æŠï¼ˆã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã¯å®Ÿéš›ã®ãƒšãƒ¼ã‚¸ã«å¿œã˜ã¦èª¿æ•´ãŒå¿…è¦ï¼‰
        news_items = soup.select('div.NewsList_item')[:top_n]

        results = []
        for item in news_items:
            title_tag = item.select_one('a.NewsList_link')
            title = title_tag.get_text(strip=True) if title_tag else 'ç„¡é¡Œ'

            time_tag = item.select_one('time')
            timestamp = time_tag['datetime'] if time_tag and time_tag.has_attr('datetime') else 'ç„¡æ™‚é–“'

            # è·å–æ–°é—»é“¾æ¥
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªãƒ³ã‚¯ã‚’å–å¾—
            link = title_tag['href'] if title_tag and title_tag.has_attr('href') else ''
            if link and not link.startswith('http'):
                link = 'https://news.yahoo.co.jp' + link

            results.append({
                'category': category_name,
                'title': title,
                'timestamp': timestamp,
                'link': link
            })

        return results
        
    except Exception as e:
        print(f"åˆ†é¡ã€Œ{category_name}ã€ã®å–å¾—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
        return []

def save_to_json(data: List[Dict], filename: str = None) -> str:
    """
    Save news data to JSON file
    ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"yahoo_news_{timestamp}.json"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
    output_dir = 'yahoo_news_dataset'
    os.makedirs(output_dir, exist_ok=True)
    
    filepath = os.path.join(output_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return filepath

def main():
    """
    Main function to crawl all categories
    ã™ã¹ã¦ã®ã‚«ãƒ†ã‚´ãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    print("ğŸš€ Starting Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ Crawler...")
    print("=" * 50)
    
    all_news = []
    
    for category, url in category_urls.items():
        print(f"ğŸ”„ Crawling category: {category}")
        try:
            news = fetch_top_news(category, url)
            all_news.extend(news)
            print(f"   âœ… Found {len(news)} news items")
        except Exception as e:
            print(f"   âŒ Error: {e}")
    
    print(f"\nğŸ“Š Total news collected: {len(all_news)}")
    
    # æ˜¾ç¤ºæ”¶é›†åˆ°çš„æ–°é—»
    # åé›†ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¡¨ç¤º
    print("\nğŸ“° Collected News:")
    print("-" * 50)
    for news in all_news:
        print(f"[{news['category']}] {news['timestamp']} - {news['title']}")
        if news['link']:
            print(f"    Link: {news['link']}")
        print()
    
    # ä¿å­˜åˆ°JSONæ–‡ä»¶
    # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    if all_news:
        filename = save_to_json(all_news)
        print(f"ğŸ’¾ News data saved to: {filename}")
    else:
        print("âš ï¸  No news data to save")

if __name__ == "__main__":
    main()
