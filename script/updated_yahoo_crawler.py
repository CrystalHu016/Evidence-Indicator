#!/usr/bin/env python3
"""
Updated Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ Crawler with correct selectors
æ­£ã—ã„ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’ä½¿ç”¨ã—ãŸæ›´æ–°ã•ã‚ŒãŸYahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼
"""

import requests
from bs4 import BeautifulSoup
import json
import os
import time
from datetime import datetime
from typing import List, Dict

# Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸»è¦åˆ†ç±»URL
category_urls = {
    'å›½å†…': 'https://news.yahoo.co.jp/categories/domestic',
    'å›½éš›': 'https://news.yahoo.co.jp/categories/world',
    'çµŒæ¸ˆ': 'https://news.yahoo.co.jp/categories/business',
    'ã‚¨ãƒ³ã‚¿ãƒ¡': 'https://news.yahoo.co.jp/categories/entertainment',
    'ã‚¹ãƒãƒ¼ãƒ„': 'https://news.yahoo.co.jp/categories/sports',
    'ITãƒ»ç§‘å­¦': 'https://news.yahoo.co.jp/categories/it',
    'ãƒ©ã‚¤ãƒ•': 'https://news.yahoo.co.jp/categories/life',
    'åœ°åŸŸ': 'https://news.yahoo.co.jp/categories/local',
}

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

def fetch_top_news(category_name: str, url: str, top_n: int = 2) -> List[Dict]:
    """
    Fetch top news from a specific category with multiple selector strategies
    è¤‡æ•°ã®ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼æˆ¦ç•¥ã‚’ä½¿ç”¨ã—ã¦ç‰¹å®šã®ã‚«ãƒ†ã‚´ãƒªã‹ã‚‰ãƒˆãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
    """
    try:
        print(f"    Fetching: {url}")
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        results = []
        
        # Strategy 1: Try multiple possible selectors
        # æˆ¦ç•¥1: è¤‡æ•°ã®å¯èƒ½ãªã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã™
        selectors_to_try = [
            'div.NewsList_item',
            'li.NewsList_item',
            'div[class*="NewsList"]',
            'div[class*="news"]',
            'div[class*="item"]',
            'article',
            'div.news-item',
            'div.news-list-item'
        ]
        
        news_items = []
        for selector in selectors_to_try:
            items = soup.select(selector)
            if items:
                news_items = items[:top_n]
                print(f"    Found {len(items)} items using selector: {selector}")
                break
        
        if not news_items:
            # Strategy 2: Look for any links that might be news
            # æˆ¦ç•¥2: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã‚ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            all_links = soup.find_all('a', href=True)
            news_links = [link for link in all_links if '/articles/' in link.get('href', '')]
            news_items = news_links[:top_n]
            print(f"    Found {len(news_items)} news links using href search")
        
        if not news_items:
            # Strategy 3: Look for any text that might be news titles
            # æˆ¦ç•¥3: ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¿ã‚¤ãƒˆãƒ«ã§ã‚ã‚‹å¯èƒ½æ€§ã®ã‚ã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’æ¢ã™
            potential_titles = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            news_items = potential_titles[:top_n]
            print(f"    Found {len(news_items)} potential titles using heading tags")
        
        # Process found items
        # è¦‹ã¤ã‹ã£ãŸé …ç›®ã‚’å‡¦ç†
        for i, item in enumerate(news_items):
            try:
                # Extract title
                # ã‚¿ã‚¤ãƒˆãƒ«ã‚’æŠ½å‡º
                if item.name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
                    title = item.get_text(strip=True)
                    link = ''
                else:
                    title_tag = item.find('a') or item
                    title = title_tag.get_text(strip=True) if title_tag else f'ãƒ‹ãƒ¥ãƒ¼ã‚¹{i+1}'
                    link = title_tag.get('href', '') if hasattr(title_tag, 'get') else ''
                
                # Clean title
                # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if title and len(title) > 5:
                    # Extract timestamp if available
                    # åˆ©ç”¨å¯èƒ½ãªå ´åˆã¯ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã‚’æŠ½å‡º
                    timestamp = datetime.now().isoformat()
                    
                    # Look for time elements
                    # æ™‚é–“è¦ç´ ã‚’æ¢ã™
                    time_elem = item.find('time') or item.find('span', class_='time') or item.find('span', class_='date')
                    if time_elem:
                        timestamp = time_elem.get('datetime', time_elem.get_text(strip=True))
                    
                    # Build full link
                    # å®Œå…¨ãªãƒªãƒ³ã‚¯ã‚’æ§‹ç¯‰
                    if link and not link.startswith('http'):
                        link = 'https://news.yahoo.co.jp' + link
                    
                    results.append({
                        'category': category_name,
                        'title': title,
                        'timestamp': timestamp,
                        'link': link,
                        'source_url': url
                    })
                    
            except Exception as e:
                print(f"    Error processing item {i+1}: {e}")
                continue
        
        return results
        
    except Exception as e:
        print(f"    Error fetching {category_name}: {e}")
        return []

def save_to_json(data: List[Dict], filename: str = None) -> str:
    """
    Save news data to JSON file
    ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"yahoo_news_updated_{timestamp}.json"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
    output_dir = 'yahoo_news_dataset'
    os.makedirs(output_dir, exist_ok=True)
    
    filepath = os.path.join(output_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return filepath

def main():
    """
    Main function to crawl all categories
    ã™ã¹ã¦ã®ã‚«ãƒ†ã‚´ãƒªã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    print("ğŸš€ Starting Updated Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ Crawler...")
    print("=" * 60)
    
    all_news = []
    
    for category, url in category_urls.items():
        print(f"ğŸ”„ Crawling category: {category}")
        try:
            news = fetch_top_news(category, url)
            all_news.extend(news)
            print(f"   âœ… Found {len(news)} news items")
            
            # Add delay between requests
            # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«é…å»¶ã‚’è¿½åŠ 
            time.sleep(2)
            
        except Exception as e:
            print(f"   âŒ Error: {e}")
    
    print(f"\nğŸ“Š Total news collected: {len(all_news)}")
    
    # æ˜¾ç¤ºæ”¶é›†åˆ°çš„æ–°é—»
    # åé›†ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¡¨ç¤º
    if all_news:
        print("\nğŸ“° Collected News:")
        print("-" * 60)
        for news in all_news:
            print(f"[{news['category']}] {news['timestamp']} - {news['title']}")
            if news['link']:
                print(f"    Link: {news['link']}")
            print()
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        filename = save_to_json(all_news)
        print(f"ğŸ’¾ News data saved to: {filename}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        print(f"\nğŸ“ˆ Statistics:")
        print(f"   Total categories processed: {len(category_urls)}")
        print(f"   Total news items: {len(all_news)}")
        print(f"   Average per category: {len(all_news)/len(category_urls):.1f}")
        
    else:
        print("âš ï¸  No news data to save")
        print("ğŸ’¡ Tip: The page structure might have changed. Check the selectors.")

if __name__ == "__main__":
    main()
