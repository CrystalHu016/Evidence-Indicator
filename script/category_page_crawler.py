#!/usr/bin/env python3
"""
Category Page Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ Crawler with Content Extraction and LLM Analysis
å„ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã€å†…å®¹ã‚‚æŠ½å‡ºã—ã€LLMã§åˆ†æã™ã‚‹Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼
"""

import requests
from bs4 import BeautifulSoup
import json
import os
import time
from datetime import datetime
from typing import List, Dict
import openai
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Set OpenAI API key
openai.api_key = os.getenv('OPENAI_API_KEY')

# Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å„åˆ†ç±»é¡µé¢URL
category_urls = {
    'å›½å†…': 'https://news.yahoo.co.jp/categories/domestic',
    'å›½éš›': 'https://news.yahoo.co.jp/categories/world',
    'çµŒæ¸ˆ': 'https://news.yahoo.co.jp/categories/business',
    'ã‚¨ãƒ³ã‚¿ãƒ¡': 'https://news.yahoo.co.jp/categories/entertainment',
    'ã‚¹ãƒãƒ¼ãƒ„': 'https://news.yahoo.co.jp/categories/sports',
    'IT': 'https://news.yahoo.co.jp/categories/it',
    'ç§‘å­¦': 'https://news.yahoo.co.jp/categories/science',
    'ãƒ©ã‚¤ãƒ•': 'https://news.yahoo.co.jp/categories/life',
    'åœ°åŸŸ': 'https://news.yahoo.co.jp/categories/local',
}

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

def generate_queries_and_answers(content: str, category: str) -> Dict:
    """
    Generate two queries and answers using LLM based on content
    LLMã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«åŸºã¥ã„ã¦2ã¤ã®ã‚¯ã‚¨ãƒªã¨å›ç­”ã‚’ç”Ÿæˆ
    """
    try:
        if not openai.api_key:
            print("      âš ï¸  OpenAI API key not found, using fallback method")
            return generate_fallback_qa(content, category)
        
        print("      ğŸ¤– Using LLM to generate queries and answers...")
        
        # Generate two different types of queries
        # 2ã¤ã®ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
        prompt = f"""
ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®å†…å®¹ã‚’åˆ†æã—ã¦ã€2ã¤ã®ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®è³ªå•ã¨å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

è¨˜äº‹å†…å®¹:
{content[:1000]}...

ã‚«ãƒ†ã‚´ãƒª: {category}

è¦æ±‚:
1. è©³ç´°æ€§ã®è³ªå•: è¨˜äº‹ã®å…·ä½“çš„ãªäº‹å®Ÿã‚„è©³ç´°ã«ã¤ã„ã¦è³ªå•
2. è¦ç´„æ€§ã®è³ªå•: è¨˜äº‹ã®è¦ç‚¹ã‚„å…¨ä½“åƒã«ã¤ã„ã¦è³ªå•

ä¸¡æ–¹ã®è³ªå•ã¯è¨˜äº‹ã®å†…å®¹ã‹ã‚‰ç­”ãˆã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã‚‹ã‚‚ã®ã«ã—ã¦ãã ã•ã„ã€‚
å›ç­”ã¯è¨˜äº‹ã®å†…å®¹ã«åŸºã¥ã„ã¦å…·ä½“çš„ã«ç­”ãˆã¦ãã ã•ã„ã€‚

å‡ºåŠ›å½¢å¼:
{{
    "query1": "è©³ç´°æ€§ã®è³ªå•",
    "answer1": "è©³ç´°æ€§ã®è³ªå•ã¸ã®å›ç­”",
    "query2": "è¦ç´„æ€§ã®è³ªå•", 
    "answer2": "è¦ç´„æ€§ã®è³ªå•ã¸ã®å›ç­”"
}}
"""
        
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=[
                {"role": "system", "content": "ã‚ãªãŸã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’åˆ†æã—ã¦é©åˆ‡ãªè³ªå•ã¨å›ç­”ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"},
                {"role": "user", "content": prompt}
            ],
            max_tokens=800,
            temperature=0.7
        )
        
        # Parse LLM response
        llm_response = response.choices[0].message.content.strip()
        
        try:
            # Try to parse JSON response
            qa_data = json.loads(llm_response)
            return {
                'query1': qa_data.get('query1', ''),
                'answer1': qa_data.get('answer1', ''),
                'query2': qa_data.get('query2', ''),
                'answer2': qa_data.get('answer2', '')
            }
        except json.JSONDecodeError:
            print("      âš ï¸  LLM response parsing failed, using fallback")
            return generate_fallback_qa(content, category)
            
    except Exception as e:
        print(f"      âš ï¸  LLM error: {e}, using fallback method")
        return generate_fallback_qa(content, category)

def generate_fallback_qa(content: str, category: str) -> Dict:
    """
    Fallback method to generate queries and answers without LLM
    LLMãªã—ã§ã‚¯ã‚¨ãƒªã¨å›ç­”ã‚’ç”Ÿæˆã™ã‚‹ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ–¹æ³•
    """
    print("      ğŸ“ Using fallback method to generate queries and answers...")
    
    # Simple keyword-based query generation
    # ã‚·ãƒ³ãƒ—ãƒ«ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®ã‚¯ã‚¨ãƒªç”Ÿæˆ
    content_lower = content.lower()
    
    # Generate first query (detail-oriented)
    # æœ€åˆã®ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆï¼ˆè©³ç´°æŒ‡å‘ï¼‰
    if any(word in content_lower for word in ['æ­»äº¡', 'æ­»äº¡è€…', 'æ­»äº¡ã—ãŸ']):
        query1 = f"ã“ã®{category}ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§æ­»äº¡ã—ãŸäººç‰©ã®è©³ç´°ã¯ï¼Ÿ"
        answer1 = "è¨˜äº‹ã®å†…å®¹ã‹ã‚‰æ­»äº¡ã«é–¢ã™ã‚‹è©³ç´°æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    elif any(word in content_lower for word in ['é‡‘é¡', 'å††', 'ä¸‡å††', 'å„„å††']):
        query1 = f"ã“ã®{category}ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§è¨€åŠã•ã‚Œã¦ã„ã‚‹é‡‘é¡ã¯ï¼Ÿ"
        answer1 = "è¨˜äº‹å†…ã§è¨€åŠã•ã‚Œã¦ã„ã‚‹å…·ä½“çš„ãªé‡‘é¡ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    elif any(word in content_lower for word in ['æ—¥æ™‚', 'æ™‚é–“', 'æ—¥ä»˜']):
        query1 = f"ã“ã®{category}ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§è¨€åŠã•ã‚Œã¦ã„ã‚‹æ—¥æ™‚ã¯ï¼Ÿ"
        answer1 = "è¨˜äº‹å†…ã§è¨€åŠã•ã‚Œã¦ã„ã‚‹å…·ä½“çš„ãªæ—¥æ™‚ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    else:
        query1 = f"ã“ã®{category}ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å…·ä½“çš„ãªäº‹å®Ÿã¯ï¼Ÿ"
        answer1 = "è¨˜äº‹ã®å†…å®¹ã‹ã‚‰å…·ä½“çš„ãªäº‹å®Ÿã‚„è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    
    # Generate second query (summary-oriented)
    # 2ç•ªç›®ã®ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆï¼ˆè¦ç´„æŒ‡å‘ï¼‰
    query2 = f"ã“ã®{category}ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è¦ç‚¹ã¯ï¼Ÿ"
    answer2 = "è¨˜äº‹ã®å†…å®¹ã‚’è¦ç´„ã™ã‚‹ã¨ã€ä¸»è¦ãªãƒã‚¤ãƒ³ãƒˆã‚„çµè«–ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
    
    return {
        'query1': query1,
        'answer1': answer1,
        'query2': query2,
        'answer2': answer2
    }

def extract_news_content(news_url: str) -> str:
    """
    Extract full content from a news article URL
    ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹URLã‹ã‚‰å®Œå…¨ãªå†…å®¹ã‚’æŠ½å‡º
    """
    try:
        print(f"      Extracting content from: {news_url}")
        response = requests.get(news_url, headers=headers, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # Try multiple content selectors for Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹
        # Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ç”¨ã®è¤‡æ•°ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã™
        content_selectors = [
            'div.article_body',
            'div.article-body',
            'div[class*="article"]',
            'div[class*="content"]',
            'div[class*="body"]',
            'article',
            'div.news-content',
            'div.news-body'
        ]
        
        content_text = ""
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # Remove script and style elements
                # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«è¦ç´ ã‚’å‰Šé™¤
                for script in content_elem(["script", "style"]):
                    script.decompose()
                
                # Get text content
                # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
                content_text = content_elem.get_text(separator='\n', strip=True)
                if content_text and len(content_text) > 50:
                    print(f"      Found content using selector: {selector}")
                    break
        
        if not content_text:
            # Fallback: try to get any text content
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
            content_text = soup.get_text(separator='\n', strip=True)
            # Limit content length to avoid too long text
            # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é•·ã•ã‚’åˆ¶é™ã—ã¦é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’é¿ã‘ã‚‹
            if len(content_text) > 1000:
                content_text = content_text[:1000] + "..."
        
        return content_text
        
    except Exception as e:
        print(f"      Error extracting content: {e}")
        return "å†…å®¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

def fetch_category_news(category_name: str, url: str, top_n: int = 2) -> List[Dict]:
    """
    Fetch news from a specific category page with content extraction and LLM analysis
    ç‰¹å®šã®ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã€å†…å®¹ã‚‚æŠ½å‡ºã—ã€LLMã§åˆ†æ
    """
    try:
        print(f"    Fetching: {url}")
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        results = []
        
        # Look for news links with /articles/ in href
        # hrefã«/articles/ã‚’å«ã‚€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        article_links = soup.find_all('a', href=True)
        news_links = [link for link in article_links if '/articles/' in link.get('href', '')]
        
        print(f"    Found {len(news_links)} article links")
        
        # Process first top_n news links
        # æœ€åˆã®top_nå€‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªãƒ³ã‚¯ã‚’å‡¦ç†
        for i, link in enumerate(news_links[:top_n]):
            try:
                href = link.get('href', '')
                title = link.get_text(strip=True)
                
                # Clean title
                # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if title and len(title) > 5 and not title.startswith('http'):
                    # Build full URL
                    # å®Œå…¨ãªURLã‚’æ§‹ç¯‰
                    if href.startswith('/'):
                        full_url = 'https://news.yahoo.co.jp' + href
                    else:
                        full_url = href
                    
                    # Extract full content
                    # å®Œå…¨ãªå†…å®¹ã‚’æŠ½å‡º
                    content = extract_news_content(full_url)
                    
                    # Generate queries and answers using LLM
                    # LLMã‚’ä½¿ç”¨ã—ã¦ã‚¯ã‚¨ãƒªã¨å›ç­”ã‚’ç”Ÿæˆ
                    qa_data = generate_queries_and_answers(content, category_name)
                    
                    # Create first entry with query1
                    # query1ã§æœ€åˆã®ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ
                    results.append({
                        'data_id': f"{category_name}_{i+1}_1",
                        'category': category_name,
                        'title': title,
                        'link': full_url,
                        'content': content,
                        'query': qa_data['query1'],
                        'answer': qa_data['answer1'],
                        'timestamp': datetime.now().isoformat(),
                        'source': 'Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹',
                        'crawler_version': 'category_page_v3.1'
                    })
                    
                    # Create second entry with query2
                    # query2ã§2ç•ªç›®ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ
                    results.append({
                        'data_id': f"{category_name}_{i+1}_2",
                        'category': category_name,
                        'title': title,
                        'link': full_url,
                        'content': content,
                        'query': qa_data['query2'],
                        'answer': qa_data['answer2'],
                        'timestamp': datetime.now().isoformat(),
                        'source': 'Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹',
                        'crawler_version': 'category_page_v3.1'
                    })
                    
                    # Add delay between content extraction and LLM analysis requests
                    # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã¨LLMåˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«é…å»¶ã‚’è¿½åŠ 
                    time.sleep(2)
                    
            except Exception as e:
                print(f"    Error processing link {i+1}: {e}")
                continue
        
        return results
        
    except Exception as e:
        print(f"    Error fetching {category_name}: {e}")
        return []

def save_to_json(data: List[Dict], filename: str = None) -> str:
    """
    Save news data to JSON file
    ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"yahoo_news_categories_separate_entries_{timestamp}.json"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
    output_dir = 'yahoo_news_dataset'
    os.makedirs(output_dir, exist_ok=True)
    
    filepath = os.path.join(output_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return filepath

def main():
    """
    Main function to crawl all category pages with content extraction and LLM analysis
    ã™ã¹ã¦ã®ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã€å†…å®¹ã‚‚æŠ½å‡ºã—ã€LLMã§åˆ†æã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    print("ğŸš€ Starting Category Page Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ Crawler with Separate Q&A Entries...")
    print("=" * 80)
    
    # Check OpenAI API key
    if not openai.api_key:
        print("âš ï¸  OpenAI API key not found. Will use fallback method for Q&A generation.")
        print("   Set OPENAI_API_KEY environment variable for LLM analysis.")
    
    all_news = []
    
    for category, url in category_urls.items():
        print(f"ğŸ”„ Crawling category: {category}")
        try:
            news = fetch_category_news(category, url, top_n=2)
            all_news.extend(news)
            print(f"   âœ… Found {len(news)} entries with content and Q&A")
            
            # Add delay between category requests
            # ã‚«ãƒ†ã‚´ãƒªãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«é…å»¶ã‚’è¿½åŠ 
            time.sleep(3)
            
        except Exception as e:
            print(f"   âŒ Error: {e}")
    
    print(f"\nğŸ“Š Total entries collected: {len(all_news)}")
    
    # æ˜¾ç¤ºæ”¶é›†åˆ°çš„æ–°é—»
    # åé›†ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¡¨ç¤º
    if all_news:
        print("\nğŸ“° Collected News Entries:")
        print("-" * 80)
        
        # Group by category
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        categories = {}
        for news in all_news:
            cat = news['category']
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(news)
        
        # Display by category
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«è¡¨ç¤º
        for category, items in sorted(categories.items()):
            print(f"\nğŸ·ï¸  {category} ({len(items)} entries):")
            for i, news in enumerate(items, 1):
                print(f"   {i}. Data ID: {news['data_id']}")
                print(f"      Title: {news['title']}")
                if news['link']:
                    print(f"      Link: {news['link']}")
                if news.get('content'):
                    content_preview = news['content'][:100] + "..." if len(news['content']) > 100 else news['content']
                    print(f"      Content: {content_preview}")
                if news.get('query'):
                    print(f"      Query: {news['query']}")
                    print(f"      Answer: {news['answer'][:100]}...")
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        filename = save_to_json(all_news)
        print(f"\nğŸ’¾ News data with separate Q&A entries saved to: {filename}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        print(f"\nğŸ“ˆ Statistics:")
        print(f"   Total entries: {len(all_news)}")
        print(f"   Categories found: {len(categories)}")
        for cat, items in sorted(categories.items()):
            print(f"   - {cat}: {len(items)} entries")
        
        # æ˜¾ç¤ºå†…å®¹ç»Ÿè®¡
        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„çµ±è¨ˆã‚’è¡¨ç¤º
        content_lengths = [len(news.get('content', '')) for news in all_news]
        if content_lengths:
            avg_length = sum(content_lengths) / len(content_lengths)
            print(f"   Average content length: {avg_length:.0f} characters")
            print(f"   Total content size: {sum(content_lengths)} characters")
        
        # æ˜¾ç¤ºQ&Aç»Ÿè®¡
        # Q&Açµ±è¨ˆã‚’è¡¨ç¤º
        qa_count = sum(1 for news in all_news if news.get('query') and news.get('answer'))
        print(f"   Entries with Q&A generated: {qa_count}/{len(all_news)}")
        
    else:
        print("âš ï¸  No news data to save")
        print("ğŸ’¡ Tip: The page structure might have changed significantly.")

if __name__ == "__main__":
    main()
