#!/usr/bin/env python3
"""
Enhanced SmartNews Business Crawler with LLM Integration
åŸºäºSmartNews Businessç½‘ç«™çŸ¥è¯†çš„æ™ºèƒ½å›ç­”ç³»ç»Ÿ
"""

import requests
from bs4 import BeautifulSoup
import os
import json
import time
import openai
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import logging
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class SmartNewsBusinessCrawler:
    """SmartNews Businessç½‘ç«™çˆ¬è™«ä¸LLMé›†æˆç³»ç»Ÿ"""
    
    def __init__(self, openai_api_key: str = None):
        self.base_url = 'https://business.smartnews.com'
        self.newsroom_url = 'https://business.smartnews.com/newsroom'
        self.blogs_url = 'https://business.smartnews.com/newsroom/blogs'
        self.company_url = 'https://business.smartnews.com/company'
        
        # OpenAIé…ç½®
        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
        if self.openai_api_key:
            openai.api_key = self.openai_api_key
            logger.info("âœ… OpenAI API key loaded")
        else:
            logger.warning("âš ï¸ OpenAI API key not found")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = 'smartnews_dataset'
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è¯·æ±‚å¤´è®¾ç½®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # æ ¸å¿ƒæŸ¥è¯¢æ¨¡æ¿
        self.core_queries = [
            "What is SmartNews and what is their mission?",
            "What are SmartNews' company values and principles?",
            "How does SmartNews work with publishers and advertisers?",
            "What career opportunities are available at SmartNews?",
            "What is the SmartTake Newsletter and what does it offer?",
            "How does SmartNews ensure quality and trustworthy news?",
            "What makes SmartNews different from other news platforms?",
            "How does SmartNews contribute to society?",
            "What are the key features of SmartNews' business model?",
            "How does SmartNews balance editorial curation with algorithms?"
        ]
    
    def get_page_content(self, url: str, delay: float = 2.0) -> Optional[str]:
        """è·å–ç½‘é¡µå†…å®¹ï¼ŒåŒ…å«å»¶è¿Ÿå’Œé”™è¯¯å¤„ç†"""
        try:
            logger.info(f"ğŸ”„ Fetching: {url}")
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«åçˆ¬è™«
            time.sleep(delay)
            return response.text
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Error fetching {url}: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ Unexpected error: {e}")
            return None
    
    def parse_main_page_content(self, html: str) -> Dict[str, str]:
        """è§£æSmartNewsä¸»é¡µå†…å®¹"""
        soup = BeautifulSoup(html, 'html.parser')
        content = {}
        
        try:
            # æå–ä¸»è¦ä¿¡æ¯
            content['title'] = soup.find('h1').text.strip() if soup.find('h1') else "SmartNews Business"
            
            # æå–ä½¿å‘½å®£è¨€
            mission_section = soup.find('h3', string=lambda text: 'Mission' in text if text else False)
            if mission_section:
                mission_text = mission_section.find_next_sibling()
                if mission_text:
                    content['mission'] = mission_text.text.strip()
            
            # æå–å…¬å¸ä»·å€¼è§‚
            values_section = soup.find('h2', string=lambda text: 'Values' in text if text else False)
            if values_section:
                values_list = values_section.find_next_siblings()
                content['values'] = [v.text.strip() for v in values_list if v.text.strip()]
            
            # æå–å…¶ä»–å…³é”®ä¿¡æ¯
            content['description'] = soup.find('p').text.strip() if soup.find('p') else ""
            
            logger.info(f"âœ… Parsed main page content: {len(content)} sections")
            return content
            
        except Exception as e:
            logger.error(f"âŒ Error parsing main page: {e}")
            return {}
    
    def parse_newsroom_content(self, html: str) -> List[Dict[str, str]]:
        """è§£ææ–°é—»å®¤é¡µé¢å†…å®¹"""
        soup = BeautifulSoup(html, 'html.parser')
        articles = []
        
        try:
            # æŸ¥æ‰¾æ–‡ç« é“¾æ¥
            article_links = soup.find_all('a', href=True)
            
            for link in article_links:
                href = link.get('href')
                if href and '/newsroom/' in href:
                    title = link.get_text(strip=True)
                    if title and len(title) > 10:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ ‡é¢˜
                        articles.append({
                            'title': title,
                            'url': self.base_url + href if href.startswith('/') else href,
                            'type': 'newsroom'
                        })
            
            logger.info(f"âœ… Found {len(articles)} newsroom articles")
            return articles[:20]  # é™åˆ¶æ•°é‡é¿å…è¿‡å¤šè¯·æ±‚
            
        except Exception as e:
            logger.error(f"âŒ Error parsing newsroom: {e}")
            return []
    
    def parse_blog_content(self, html: str) -> List[Dict[str, str]]:
        """è§£æåšå®¢é¡µé¢å†…å®¹"""
        soup = BeautifulSoup(html, 'html.parser')
        blogs = []
        
        try:
            # æŸ¥æ‰¾åšå®¢é“¾æ¥
            blog_links = soup.find_all('a', href=True)
            
            for link in blog_links:
                href = link.get('href')
                if href and '/blogs/' in href:
                    title = link.get_text(strip=True)
                    if title and len(title) > 10:
                        blogs.append({
                            'title': title,
                            'url': self.base_url + href if href.startswith('/') else href,
                            'type': 'blog'
                        })
            
            logger.info(f"âœ… Found {len(blogs)} blog posts")
            return blogs[:20]
            
        except Exception as e:
            logger.error(f"âŒ Error parsing blogs: {e}")
            return []
    
    def extract_article_content(self, url: str) -> Optional[Dict[str, str]]:
        """æå–å…·ä½“æ–‡ç« å†…å®¹"""
        html = self.get_page_content(url, delay=1.5)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        try:
            # æå–æ ‡é¢˜
            title = soup.find('h1')
            title_text = title.text.strip() if title else "No Title"
            
            # æå–æ­£æ–‡å†…å®¹
            content_tags = soup.find_all(['p', 'h2', 'h3', 'h4', 'h5', 'h6'])
            content_text = '\n'.join([tag.text.strip() for tag in content_tags if tag.text.strip()])
            
            # æå–å‘å¸ƒæ—¶é—´
            time_tag = soup.find('time')
            publish_time = time_tag.get('datetime') if time_tag else None
            
            return {
                'title': title_text,
                'content': content_text,
                'url': url,
                'publish_time': publish_time,
                'word_count': len(content_text.split())
            }
            
        except Exception as e:
            logger.error(f"âŒ Error extracting article content from {url}: {e}")
            return None
    
    def generate_llm_answer(self, query: str, context_content: str) -> Optional[str]:
        """ä½¿ç”¨LLMåŸºäºç½‘ç«™å†…å®¹ç”Ÿæˆå›ç­”"""
        if not self.openai_api_key:
            logger.warning("âš ï¸ No OpenAI API key, skipping LLM generation")
            return None
        
        try:
            prompt = f"""
            åŸºäºä»¥ä¸‹SmartNews Businessç½‘ç«™çš„å†…å®¹ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ã€‚
            
            ç½‘ç«™å†…å®¹ï¼š
            {context_content[:3000]}  # é™åˆ¶é•¿åº¦é¿å…è¶…å‡ºtokené™åˆ¶
            
            ç”¨æˆ·é—®é¢˜ï¼š{query}
            
            è¦æ±‚ï¼š
            1. åªä½¿ç”¨ä¸Šè¿°ç½‘ç«™å†…å®¹ä½œä¸ºä¿¡æ¯æº
            2. å¦‚æœç½‘ç«™å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜
            3. æä¾›å‡†ç¡®ã€æœ‰æ ¹æ®çš„å›ç­”
            4. å›ç­”è¦ç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹
            5. ä½¿ç”¨ä¸­æ–‡å›ç­”
            
            å›ç­”ï¼š
            """
            
            response = openai.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å•†ä¸šåˆ†æå¸ˆï¼Œæ“…é•¿åŸºäºSmartNews Businessç½‘ç«™å†…å®¹å›ç­”é—®é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            answer = response.choices[0].message.content.strip()
            logger.info(f"âœ… Generated LLM answer for: {query[:50]}...")
            return answer
            
        except Exception as e:
            logger.error(f"âŒ Error generating LLM answer: {e}")
            return None
    
    def create_dataset_entry(self, query: str, answer: str, urls: List[str], 
                           content_summary: str) -> Dict[str, any]:
        """åˆ›å»ºæ•°æ®é›†æ¡ç›®"""
        return {
            "core_query": query,
            "answer": answer,
            "original_urls": urls,
            "content_summary": content_summary,
            "timestamp": datetime.now().isoformat(),
            "source": "SmartNews Business",
            "crawler_version": "enhanced_v1.0"
        }
    
    def crawl_and_generate_dataset(self) -> List[Dict[str, any]]:
        """ä¸»è¦çˆ¬å–å’Œæ•°æ®é›†ç”Ÿæˆæµç¨‹"""
        logger.info("ğŸš€ Starting SmartNews Business dataset generation...")
        
        dataset = []
        
        # 1. çˆ¬å–ä¸»é¡µå†…å®¹
        main_html = self.get_page_content(self.base_url)
        if main_html:
            main_content = self.parse_main_page_content(main_html)
            logger.info("âœ… Main page content parsed")
        else:
            logger.error("âŒ Failed to fetch main page")
            return dataset
        
        # 2. çˆ¬å–æ–°é—»å®¤å†…å®¹
        newsroom_html = self.get_page_content(self.newsroom_url)
        newsroom_articles = []
        if newsroom_html:
            newsroom_articles = self.parse_newsroom_content(newsroom_html)
        
        # 3. çˆ¬å–åšå®¢å†…å®¹
        blogs_html = self.get_page_content(self.blogs_url)
        blog_posts = []
        if blogs_html:
            blog_posts = self.blogs_url
        
        # 4. åˆå¹¶æ‰€æœ‰å†…å®¹
        all_content = {
            'main_page': main_content,
            'newsroom': newsroom_articles,
            'blogs': blog_posts
        }
        
        # 5. ä¸ºæ¯ä¸ªæ ¸å¿ƒæŸ¥è¯¢ç”Ÿæˆå›ç­”
        for query in self.core_queries:
            logger.info(f"ğŸ” Processing query: {query[:50]}...")
            
            # ç”ŸæˆLLMå›ç­”
            answer = self.generate_llm_answer(query, str(all_content))
            
            if answer:
                # åˆ›å»ºæ•°æ®é›†æ¡ç›®
                entry = self.create_dataset_entry(
                    query=query,
                    answer=answer,
                    urls=[self.base_url, self.newsroom_url, self.blogs_url],
                    content_summary=f"Content from main page, newsroom, and blogs"
                )
                
                dataset.append(entry)
                logger.info(f"âœ… Created dataset entry for: {query[:30]}...")
            else:
                logger.warning(f"âš ï¸ Failed to generate answer for: {query[:30]}...")
        
        logger.info(f"ğŸ‰ Dataset generation completed! Created {len(dataset)} entries")
        return dataset
    
    def save_dataset(self, dataset: List[Dict[str, any]], filename: str = None):
        """ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"smartnews_dataset_{timestamp}.json"
        
        filepath = os.path.join(self.output_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(dataset, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… Dataset saved to: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"âŒ Error saving dataset: {e}")
            return None
    
    def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„çˆ¬å–å’Œæ•°æ®é›†ç”Ÿæˆæµç¨‹"""
        try:
            # 1. çˆ¬å–å†…å®¹å¹¶ç”Ÿæˆæ•°æ®é›†
            dataset = self.crawl_and_generate_dataset()
            
            if dataset:
                # 2. ä¿å­˜æ•°æ®é›†
                filepath = self.save_dataset(dataset)
                
                # 3. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
                logger.info("ğŸ“Š Dataset Statistics:")
                logger.info(f"   Total entries: {len(dataset)}")
                logger.info(f"   Core queries: {len(self.core_queries)}")
                logger.info(f"   Output file: {filepath}")
                
                return filepath
            else:
                logger.error("âŒ No dataset generated")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Pipeline failed: {e}")
            return None


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ SmartNews Business Enhanced Crawler with LLM Integration")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âš ï¸  Warning: OPENAI_API_KEY not found in environment variables")
        print("   The crawler will work but LLM answer generation will be skipped")
        print("   Please set OPENAI_API_KEY in your .env file")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = SmartNewsBusinessCrawler(api_key)
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    result = crawler.run_full_pipeline()
    
    if result:
        print(f"\nğŸ‰ Success! Dataset saved to: {result}")
        print("ğŸ“ Check the 'smartnews_dataset' folder for your generated dataset")
    else:
        print("\nâŒ Failed to generate dataset. Check the logs above for details.")


if __name__ == "__main__":
    main()
