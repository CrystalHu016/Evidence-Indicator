#!/usr/bin/env python3
"""
Enhanced Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ Crawler with LLM Integration
åŸºäºYahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ç½‘ç«™çŸ¥è¯†çš„æ™ºèƒ½å›ç­”ç³»ç»Ÿ
"""

import requests
from bs4 import BeautifulSoup
import os
import json
import time
import openai
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import logging
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class YahooNewsCrawler:
    """Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ç½‘ç«™çˆ¬è™«ä¸LLMé›†æˆç³»ç»Ÿ"""
    
    def __init__(self, openai_api_key: str = None):
        self.base_url = 'https://news.yahoo.co.jp'
        self.topics_url = 'https://news.yahoo.co.jp/topics'
        self.ranking_url = 'https://news.yahoo.co.jp/ranking'
        # ä¿®å¤åˆ†ç±»é¡µé¢URL - ä½¿ç”¨æ­£ç¡®çš„æ ¼å¼
        self.domestic_url = 'https://news.yahoo.co.jp/categories/domestic'
        self.international_url = 'https://news.yahoo.co.jp/categories/international'
        self.economy_url = 'https://news.yahoo.co.jp/categories/economy'
        self.entertainment_url = 'https://news.yahoo.co.jp/categories/entertainment'
        self.sports_url = 'https://news.yahoo.co.jp/categories/sports'
        self.it_url = 'https://news.yahoo.co.jp/categories/it'
        self.science_url = 'https://news.yahoo.co.jp/categories/science'
        self.life_url = 'https://news.yahoo.co.jp/categories/life'
        
        # OpenAIé…ç½®
        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
        if self.openai_api_key:
            openai.api_key = self.openai_api_key
            logger.info("âœ… OpenAI API key loaded")
        else:
            logger.warning("âš ï¸ OpenAI API key not found")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = 'yahoo_news_dataset'
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è¯·æ±‚å¤´è®¾ç½®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # æ ¸å¿ƒæŸ¥è¯¢æ¨¡æ¿
        self.core_queries = [
            "What are the main news categories on Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹?",
            "What are the current trending topics on Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹?",
            "How does Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ organize its news content?",
            "What types of news does Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ cover?",
            "What is Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ Live and how does it work?",
            "What are the ranking features on Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹?",
            "How does Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ recommend content to users?",
            "What are the comment and opinion features?",
            "How does Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ ensure news quality?",
            "What are the sources of news on Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹?",
            "How does Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ handle breaking news?",
            "What makes Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ different from other news platforms?",
            "What are the main features of Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹?",
            "How does Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ organize news by category?",
            "What are the trending topics and rankings?",
            "How does Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ present news to users?"
        ]
    
    def get_page_content(self, url: str, delay: float = 2.0) -> Optional[str]:
        """è·å–ç½‘é¡µå†…å®¹ï¼ŒåŒ…å«å»¶è¿Ÿå’Œé”™è¯¯å¤„ç†"""
        try:
            logger.info(f"ğŸ”„ Fetching: {url}")
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«åçˆ¬è™«
            time.sleep(delay)
            return response.text
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Error fetching {url}: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ Unexpected error: {e}")
            return None
    
    def parse_main_page_content(self, html: str) -> Dict[str, str]:
        """è§£æYahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸»é¡µå†…å®¹"""
        soup = BeautifulSoup(html, 'html.parser')
        content = {}
        
        try:
            # æå–ä¸»è¦ä¿¡æ¯
            title_tag = soup.find('h1')
            content['title'] = title_tag.text.strip() if title_tag else "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹"
            
            # æå–æ–°é—»åˆ†ç±»
            category_links = soup.find_all('a', href=True)
            categories = []
            for link in category_links:
                href = link.get('href')
                if href and any(cat in href for cat in ['/domestic', '/international', '/economy', '/sports', '/entertainment']):
                    categories.append(link.text.strip())
            content['categories'] = categories[:10]  # é™åˆ¶æ•°é‡
            
            # æå–çƒ­é—¨è¯é¢˜
            topics = soup.find_all('a', href=True)
            trending_topics = []
            for topic in topics:
                if topic.text.strip() and len(topic.text.strip()) > 5:
                    trending_topics.append(topic.text.strip())
            content['trending_topics'] = trending_topics[:15]  # é™åˆ¶æ•°é‡
            
            # æå–å…¶ä»–å…³é”®ä¿¡æ¯
            content['description'] = "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ - æ—¥æœ¬ä¸»è¦æ–°é—»é—¨æˆ·ç½‘ç«™"
            
            logger.info(f"âœ… Parsed main page content: {len(content)} sections")
            return content
            
        except Exception as e:
            logger.error(f"âŒ Error parsing main page: {e}")
            return {}
    
    def parse_topics_content(self, html: str) -> List[Dict[str, str]]:
        """è§£æè¯é¢˜é¡µé¢å†…å®¹"""
        soup = BeautifulSoup(html, 'html.parser')
        topics = []
        
        try:
            # æŸ¥æ‰¾è¯é¢˜é“¾æ¥
            topic_links = soup.find_all('a', href=True)
            
            for link in topic_links:
                href = link.get('href')
                if href and '/topics/' in href:
                    title = link.get_text(strip=True)
                    if title and len(title) > 5:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ ‡é¢˜
                        topics.append({
                            'title': title,
                            'url': self.base_url + href if href.startswith('/') else href,
                            'type': 'topic'
                        })
            
            logger.info(f"âœ… Found {len(topics)} topics")
            return topics[:20]  # é™åˆ¶æ•°é‡é¿å…è¿‡å¤šè¯·æ±‚
            
        except Exception as e:
            logger.error(f"âŒ Error parsing topics: {e}")
            return []
    
    def parse_ranking_content(self, html: str) -> List[Dict[str, str]]:
        """è§£ææ’åé¡µé¢å†…å®¹"""
        soup = BeautifulSoup(html, 'html.parser')
        rankings = []
        
        try:
            # æŸ¥æ‰¾æ’åå†…å®¹
            ranking_items = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
            
            for item in ranking_items:
                text = item.get_text(strip=True)
                if text and any(keyword in text for keyword in ['ãƒ©ãƒ³ã‚­ãƒ³ã‚°', '1ä½', '2ä½', '3ä½', 'ã‚¢ã‚¯ã‚»ã‚¹']):
                    rankings.append({
                        'title': text,
                        'type': 'ranking',
                        'content': text
                    })
            
            logger.info(f"âœ… Found {len(rankings)} ranking items")
            return rankings[:15]
            
        except Exception as e:
            logger.error(f"âŒ Error parsing rankings: {e}")
            return []
    
    def parse_category_content(self, html: str, category: str) -> List[Dict[str, str]]:
        """è§£æç‰¹å®šåˆ†ç±»é¡µé¢å†…å®¹"""
        soup = BeautifulSoup(html, 'html.parser')
        articles = []
        
        try:
            # æŸ¥æ‰¾æ–‡ç« é“¾æ¥
            article_links = soup.find_all('a', href=True)
            
            for link in article_links:
                href = link.get('href')
                if href and '/articles/' in href:
                    title = link.get_text(strip=True)
                    if title and len(title) > 5:
                        articles.append({
                            'title': title,
                            'url': self.base_url + href if href.startswith('/') else href,
                            'type': 'article',
                            'category': category
                        })
            
            logger.info(f"âœ… Found {len(articles)} articles in {category}")
            return articles[:15]
            
        except Exception as e:
            logger.error(f"âŒ Error parsing {category}: {e}")
            return []
    
    def extract_article_content(self, url: str) -> Optional[Dict[str, str]]:
        """æå–å…·ä½“æ–‡ç« å†…å®¹"""
        html = self.get_page_content(url, delay=1.5)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        try:
            # æå–æ ‡é¢˜
            title = soup.find('h1')
            title_text = title.text.strip() if title else "No Title"
            
            # æå–æ­£æ–‡å†…å®¹
            content_tags = soup.find_all(['p', 'h2', 'h3', 'h4', 'h5', 'h6'])
            content_text = '\n'.join([tag.text.strip() for tag in content_tags if tag.text.strip()])
            
            # æå–å‘å¸ƒæ—¶é—´
            time_tag = soup.find('time')
            publish_time = time_tag.get('datetime') if time_tag else None
            
            return {
                'title': title_text,
                'content': content_text,
                'url': url,
                'publish_time': publish_time,
                'word_count': len(content_text.split())
            }
            
        except Exception as e:
            logger.error(f"âŒ Error extracting article content from {url}: {e}")
            return None
    
    def generate_llm_answer(self, query: str, context_content: str) -> Optional[str]:
        """ä½¿ç”¨LLMåŸºäºç½‘ç«™å†…å®¹ç”Ÿæˆå›ç­”"""
        if not self.openai_api_key:
            logger.warning("âš ï¸ No OpenAI API key, skipping LLM generation")
            return None
        
        try:
            prompt = f"""
            åŸºäºä»¥ä¸‹Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ç½‘ç«™çš„å†…å®¹ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ã€‚
            
            ç½‘ç«™å†…å®¹ï¼š
            {context_content[:3000]}  # é™åˆ¶é•¿åº¦é¿å…è¶…å‡ºtokené™åˆ¶
            
            ç”¨æˆ·é—®é¢˜ï¼š{query}
            
            è¦æ±‚ï¼š
            1. åªä½¿ç”¨ä¸Šè¿°ç½‘ç«™å†…å®¹ä½œä¸ºä¿¡æ¯æº
            2. å¦‚æœç½‘ç«™å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜
            3. æä¾›å‡†ç¡®ã€æœ‰æ ¹æ®çš„å›ç­”
            4. å›ç­”è¦ç®€æ´æ˜äº†ï¼Œçªå‡ºé‡ç‚¹
            5. ä½¿ç”¨ä¸­æ–‡å›ç­”
            
            å›ç­”ï¼š
            """
            
            response = openai.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–°é—»åˆ†æå¸ˆï¼Œæ“…é•¿åŸºäºYahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ç½‘ç«™å†…å®¹å›ç­”é—®é¢˜ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.3
            )
            
            answer = response.choices[0].message.content.strip()
            logger.info(f"âœ… Generated LLM answer for: {query[:50]}...")
            return answer
            
        except Exception as e:
            logger.error(f"âŒ Error generating LLM answer: {e}")
            return None
    
    def create_dataset_entry(self, query: str, answer: str, urls: List[str], 
                           content_summary: str) -> Dict[str, any]:
        """åˆ›å»ºæ•°æ®é›†æ¡ç›®"""
        return {
            "core_query": query,
            "answer": answer,
            "original_urls": urls,
            "content_summary": content_summary,
            "timestamp": datetime.now().isoformat(),
            "source": "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹",
            "crawler_version": "enhanced_v1.0"
        }
    
    def crawl_and_generate_dataset(self) -> List[Dict[str, any]]:
        """ä¸»è¦çˆ¬å–å’Œæ•°æ®é›†ç”Ÿæˆæµç¨‹"""
        logger.info("ğŸš€ Starting Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ dataset generation...")
        
        dataset = []
        
        # 1. çˆ¬å–ä¸»é¡µå†…å®¹
        main_html = self.get_page_content(self.base_url)
        if main_html:
            main_content = self.parse_main_page_content(main_html)
            logger.info("âœ… Main page content parsed")
        else:
            logger.error("âŒ Failed to fetch main page")
            return dataset
        
        # 2. çˆ¬å–è¯é¢˜é¡µé¢å†…å®¹
        topics_html = self.get_page_content(self.topics_url)
        topics_content = []
        if topics_html:
            topics_content = self.parse_topics_content(topics_html)
        
        # 3. çˆ¬å–æ’åé¡µé¢å†…å®¹
        ranking_html = self.get_page_content(self.ranking_url)
        ranking_content = []
        if ranking_html:
            ranking_content = self.parse_ranking_content(ranking_html)
        
        # 4. çˆ¬å–åˆ†ç±»é¡µé¢å†…å®¹
        category_urls = [
            (self.domestic_url, 'domestic'),
            (self.international_url, 'international'),
            (self.economy_url, 'economy'),
            (self.sports_url, 'sports'),
            (self.entertainment_url, 'entertainment')
        ]
        
        category_content = {}
        for url, category in category_urls:
            html = self.get_page_content(url)
            if html:
                category_content[category] = self.parse_category_content(html, category)
        
        # 5. åˆå¹¶æ‰€æœ‰å†…å®¹
        all_content = {
            'main_page': main_content,
            'topics': topics_content,
            'rankings': ranking_content,
            'categories': category_content
        }
        
        # 6. ä¸ºæ¯ä¸ªæ ¸å¿ƒæŸ¥è¯¢ç”Ÿæˆå›ç­”
        for query in self.core_queries:
            logger.info(f"ğŸ” Processing query: {query[:50]}...")
            
            # ç”ŸæˆLLMå›ç­”
            answer = self.generate_llm_answer(query, str(all_content))
            
            if answer:
                # åˆ›å»ºæ•°æ®é›†æ¡ç›®
                entry = self.create_dataset_entry(
                    query=query,
                    answer=answer,
                    urls=[self.base_url, self.topics_url, self.ranking_url],
                    content_summary=f"Content from main page, topics, rankings, and category pages"
                )
                
                dataset.append(entry)
                logger.info(f"âœ… Created dataset entry for: {query[:30]}...")
            else:
                logger.warning(f"âš ï¸ Failed to generate answer for: {query[:30]}...")
        
        logger.info(f"ğŸ‰ Dataset generation completed! Created {len(dataset)} entries")
        return dataset
    
    def save_dataset(self, dataset: List[Dict[str, any]], filename: str = None):
        """ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"yahoo_news_dataset_{timestamp}.json"
        
        filepath = os.path.join(self.output_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(dataset, f, ensure_ascii=False, indent=2)
            
            logger.info(f"âœ… Dataset saved to: {filepath}")
            return filepath
            
        except Exception as e:
            logger.error(f"âŒ Error saving dataset: {e}")
            return None
    
    def run_full_pipeline(self):
        """è¿è¡Œå®Œæ•´çš„çˆ¬å–å’Œæ•°æ®é›†ç”Ÿæˆæµç¨‹"""
        try:
            # 1. çˆ¬å–å†…å®¹å¹¶ç”Ÿæˆæ•°æ®é›†
            dataset = self.crawl_and_generate_dataset()
            
            if dataset:
                # 2. ä¿å­˜æ•°æ®é›†
                filepath = self.save_dataset(dataset)
                
                # 3. è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
                logger.info("ğŸ“Š Dataset Statistics:")
                logger.info(f"   Total entries: {len(dataset)}")
                logger.info(f"   Core queries: {len(self.core_queries)}")
                logger.info(f"   Output file: {filepath}")
                
                return filepath
            else:
                logger.error("âŒ No dataset generated")
                return None
                
        except Exception as e:
            logger.error(f"âŒ Pipeline failed: {e}")
            return None


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ Enhanced Crawler with LLM Integration")
    print("=" * 60)
    
    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âš ï¸  Warning: OPENAI_API_KEY not found in environment variables")
        print("   The crawler will work but LLM answer generation will be skipped")
        print("   Please set OPENAI_API_KEY in your .env file")
    
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = YahooNewsCrawler(api_key)
    
    # è¿è¡Œå®Œæ•´æµç¨‹
    result = crawler.run_full_pipeline()
    
    if result:
        print(f"\nğŸ‰ Success! Dataset saved to: {result}")
        print("ğŸ“ Check the 'yahoo_news_dataset' folder for your generated dataset")
    else:
        print("\nâŒ Failed to generate dataset. Check the logs above for details.")


if __name__ == "__main__":
    main()
