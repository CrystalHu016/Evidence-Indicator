#!/usr/bin/env python3
"""
Main Page Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ Crawler with URL-based category detection
URLãƒ™ãƒ¼ã‚¹ã®ã‚«ãƒ†ã‚´ãƒªæ¤œå‡ºã‚’ä½¿ç”¨ã—ãŸãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼
"""

import requests
from bs4 import BeautifulSoup
import json
import os
import time
from datetime import datetime
from typing import List, Dict

# Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ä¸»é¡µ
main_url = 'https://news.yahoo.co.jp'

headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
    'Accept-Language': 'ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7',
    'Accept-Encoding': 'gzip, deflate',
    'Connection': 'keep-alive',
    'Upgrade-Insecure-Requests': '1',
}

def get_category_from_url(url: str) -> str:
    """
    Extract category from URL path
    URLãƒ‘ã‚¹ã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’æŠ½å‡º
    """
    if '/categories/' in url:
        cat_part = url.split('/categories/')[-1].split('/')[0]
        category_map = {
            'domestic': 'å›½å†…',
            'world': 'å›½éš›',
            'business': 'çµŒæ¸ˆ',
            'entertainment': 'ã‚¨ãƒ³ã‚¿ãƒ¡',
            'sports': 'ã‚¹ãƒãƒ¼ãƒ„',
            'it': 'ITãƒ»ç§‘å­¦',
            'life': 'ãƒ©ã‚¤ãƒ•',
            'local': 'åœ°åŸŸ'
        }
        return category_map.get(cat_part, 'ä¸»è¦')
    elif '/articles/' in url:
        return 'ä¸»è¦'
    else:
        return 'ä¸»è¦'

def fetch_main_page_news() -> List[Dict]:
    """
    Fetch news from the main page
    ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
    """
    try:
        print(f"ğŸ”„ Fetching main page: {main_url}")
        response = requests.get(main_url, headers=headers, timeout=30)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')

        results = []
        
        # Strategy 1: Look for news links with /articles/ in href
        # æˆ¦ç•¥1: hrefã«/articles/ã‚’å«ã‚€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªãƒ³ã‚¯ã‚’æ¢ã™
        print("    Strategy 1: Looking for article links...")
        article_links = soup.find_all('a', href=True)
        news_links = [link for link in article_links if '/articles/' in link.get('href', '')]
        
        print(f"    Found {len(news_links)} article links")
        
        # Process first 20 news links
        # æœ€åˆã®20å€‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªãƒ³ã‚¯ã‚’å‡¦ç†
        for i, link in enumerate(news_links[:20]):
            try:
                href = link.get('href', '')
                title = link.get_text(strip=True)
                
                # Clean title
                # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if title and len(title) > 5 and not title.startswith('http'):
                    # Build full URL
                    # å®Œå…¨ãªURLã‚’æ§‹ç¯‰
                    if href.startswith('/'):
                        full_url = main_url + href
                    else:
                        full_url = href
                    
                    # Get category from URL
                    # URLã‹ã‚‰ã‚«ãƒ†ã‚´ãƒªã‚’å–å¾—
                    category = get_category_from_url(full_url)
                    
                    results.append({
                        'category': category,
                        'title': title,
                        'link': full_url,
                        'timestamp': datetime.now().isoformat(),
                        'source': 'Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹',
                        'crawler_version': 'main_page_v1.1'
                    })
                    
            except Exception as e:
                print(f"    Error processing link {i+1}: {e}")
                continue
        
        # Strategy 2: Look for trending topics
        # æˆ¦ç•¥2: ãƒˆãƒ¬ãƒ³ãƒ‰ãƒˆãƒ”ãƒƒã‚¯ã‚’æ¢ã™
        print("    Strategy 2: Looking for trending topics...")
        trending_selectors = [
            'div[class*="trending"]',
            'div[class*="topic"]',
            'div[class*="popular"]',
            'ul[class*="topic"]',
            'div[class*="ranking"]'
        ]
        
        for selector in trending_selectors:
            trending_items = soup.select(selector)
            if trending_items:
                print(f"    Found trending items using: {selector}")
                for item in trending_items[:5]:
                    try:
                        title_elem = item.find('a') or item
                        title = title_elem.get_text(strip=True) if title_elem else ''
                        
                        if title and len(title) > 5:
                            results.append({
                                'category': 'ãƒˆãƒ¬ãƒ³ãƒ‰',
                                'title': title,
                                'link': '',
                                'timestamp': datetime.now().isoformat(),
                                'source': 'Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹',
                                'crawler_version': 'main_page_v1.1'
                            })
                    except Exception as e:
                        continue
                break
        
        return results
        
    except Exception as e:
        print(f"    Error fetching main page: {e}")
        return []

def save_to_json(data: List[Dict], filename: str = None) -> str:
    """
    Save news data to JSON file
    ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
    """
    if not filename:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"yahoo_news_main_page_{timestamp}.json"
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    # å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã™ã‚‹ã“ã¨ã‚’ç¢ºèª
    output_dir = 'yahoo_news_dataset'
    os.makedirs(output_dir, exist_ok=True)
    
    filepath = os.path.join(output_dir, filename)
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    return filepath

def main():
    """
    Main function to crawl main page news
    ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    print("ğŸš€ Starting Main Page Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ Crawler (URL-based categories)...")
    print("=" * 70)
    
    # Fetch news from main page
    # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
    news_items = fetch_main_page_news()
    
    print(f"\nğŸ“Š Total news collected: {len(news_items)}")
    
    # æ˜¾ç¤ºæ”¶é›†åˆ°çš„æ–°é—»
    # åé›†ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’è¡¨ç¤º
    if news_items:
        print("\nğŸ“° Collected News:")
        print("-" * 70)
        
        # Group by category
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        categories = {}
        for news in news_items:
            cat = news['category']
            if cat not in categories:
                categories[cat] = []
            categories[cat].append(news)
        
        # Display by category
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«è¡¨ç¤º
        for category, items in sorted(categories.items()):
            print(f"\nğŸ·ï¸  {category} ({len(items)} items):")
            for i, news in enumerate(items, 1):
                print(f"   {i}. {news['title']}")
                if news['link']:
                    print(f"      Link: {news['link']}")
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶
        # JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        filename = save_to_json(news_items)
        print(f"\nğŸ’¾ News data saved to: {filename}")
        
        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        # çµ±è¨ˆæƒ…å ±ã‚’è¡¨ç¤º
        print(f"\nğŸ“ˆ Statistics:")
        print(f"   Total news items: {len(news_items)}")
        print(f"   Categories found: {len(categories)}")
        for cat, items in sorted(categories.items()):
            print(f"   - {cat}: {len(items)} items")
        
    else:
        print("âš ï¸  No news data to save")
        print("ğŸ’¡ Tip: The page structure might have changed significantly.")

if __name__ == "__main__":
    main()
