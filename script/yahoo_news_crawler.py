#!/usr/bin/env python3
"""
Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ Universal Crawler with LLM Integration
Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ - LLMè³ªå•å¿œç­”ç”Ÿæˆæ©Ÿèƒ½ä»˜ã

Features:
- Multi-category news extraction from Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹
- LLM-based Q&A generation (OpenAI GPT-3.5-turbo)
- Diverse question patterns using varied interrogative words
- 100% LLM-generated responses (no template fallbacks)
- Structured JSON output for RAG systems
"""

import requests
from bs4 import BeautifulSoup
import json
import os
import time
from datetime import datetime
from typing import List, Dict, Optional
import openai
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

class YahooNewsCrawler:
    """Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹çµ±åˆã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼"""
    
    def __init__(self):
        # OpenAI API key will be loaded from environment variable OPENAI_API_KEY
        # The new OpenAI client automatically reads from environment
        
        # Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹å„ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸URL
        self.category_urls = {
            'å›½å†…': 'https://news.yahoo.co.jp/categories/domestic',
            'å›½éš›': 'https://news.yahoo.co.jp/categories/world', 
            'çµŒæ¸ˆ': 'https://news.yahoo.co.jp/categories/business',
            'ã‚¨ãƒ³ã‚¿ãƒ¡': 'https://news.yahoo.co.jp/categories/entertainment',
            'ã‚¹ãƒãƒ¼ãƒ„': 'https://news.yahoo.co.jp/categories/sports',
            'IT': 'https://news.yahoo.co.jp/categories/it',
            'ç§‘å­¦': 'https://news.yahoo.co.jp/categories/science',
            'ãƒ©ã‚¤ãƒ•': 'https://news.yahoo.co.jp/categories/life',
            'åœ°åŸŸ': 'https://news.yahoo.co.jp/categories/local',
        }
        
        # HTTP request headers
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/115.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
        
        # Output directory
        self.output_dir = '../data/yahoo_news_dataset'
        os.makedirs(self.output_dir, exist_ok=True)

    def generate_queries_and_answers(self, content: str, category: str) -> Dict:
        """
        Generate two queries and answers using LLM based on content
        LLMã‚’ä½¿ç”¨ã—ã¦ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã«åŸºã¥ã„ã¦2ã¤ã®ã‚¯ã‚¨ãƒªã¨å›ç­”ã‚’ç”Ÿæˆ
        """
        if not os.getenv('OPENAI_API_KEY'):
            print("      âŒ OpenAI API key not found. Cannot generate Q&A without LLM.")
            return {
                'query1': '',
                'answer1': '',
                'query2': '',
                'answer2': ''
            }
        
        try:
            print("      ğŸ¤– Using LLM to generate queries and answers...")
            
            # Generate two different types of queries
            # 2ã¤ã®ç•°ãªã‚‹ã‚¿ã‚¤ãƒ—ã®ã‚¯ã‚¨ãƒªã‚’ç”Ÿæˆ
            prompt = f"""
ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®å†…å®¹ã‚’åˆ†æã—ã¦ã€2ã¤ã®å¤šæ§˜ã§å…·ä½“çš„ãªè³ªå•ã¨å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

è¨˜äº‹å†…å®¹:
{content[:1500]}...

ã‚«ãƒ†ã‚´ãƒª: {category}

è¦æ±‚:
1. è¨˜äº‹ã®å†…å®¹ã«åŸºã¥ã„ãŸå…·ä½“çš„ã§å®Ÿç”¨çš„ãªè³ªå•ã‚’ä½œæˆ
2. ã€Œãªãœã€ã€Œã©ã®ã‚ˆã†ã«ã€ã€Œã„ã¤ã€ã€Œã©ã“ã§ã€ã€Œèª°ãŒã€ã€Œä½•ã‚’ã€ãªã©ç•°ãªã‚‹ç–‘å•è©ã‚’ä½¿ç”¨
3. ä¸€èˆ¬åŒ–å¯èƒ½ã§æ•™è‚²çš„ä¾¡å€¤ã®ã‚ã‚‹è³ªå•ã‚’å„ªå…ˆ
4. åŒã˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è³ªå•ï¼ˆè¦ç´„ã€è¦ç‚¹ãªã©ï¼‰ã‚’é¿ã‘ã‚‹
5. è¨˜äº‹ã‹ã‚‰å­¦ã¹ã‚‹çŸ¥è­˜ã‚„ä»•çµ„ã¿ã«ç„¦ç‚¹ã‚’å½“ã¦ã‚‹

è³ªå•ä¾‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³:
- ã€Œã€œã®ç†ç”±ã¯ä½•ã§ã™ã‹ï¼Ÿã€
- ã€Œã€œã¯ã©ã®ã‚ˆã†ãªä»•çµ„ã¿ã§å‹•ä½œã—ã¾ã™ã‹ï¼Ÿã€  
- ã€Œã€œã™ã‚‹éš›ã«æ³¨æ„ã™ã¹ããƒã‚¤ãƒ³ãƒˆã¯ä½•ã§ã™ã‹ï¼Ÿã€
- ã€Œã€œã®èƒŒæ™¯ã«ã¯ã©ã®ã‚ˆã†ãªè¦å› ãŒã‚ã‚Šã¾ã™ã‹ï¼Ÿã€
- ã€Œã€œã«é–¢ã™ã‚‹å…·ä½“çš„ãªæ•°å€¤ã‚„è©³ç´°ã¯ï¼Ÿã€

å‡ºåŠ›å½¢å¼:
{{
    "query1": "å…·ä½“çš„ã§æ³›åŒ–å¯èƒ½ãªè³ªå•1",
    "answer1": "è³ªå•1ã¸ã®è©³ç´°ãªå›ç­”",
    "query2": "ç•°ãªã‚‹è§’åº¦ã‹ã‚‰ã®å…·ä½“çš„ãªè³ªå•2", 
    "answer2": "è³ªå•2ã¸ã®è©³ç´°ãªå›ç­”"
}}
"""
            
            client = openai.OpenAI()
            response = client.chat.completions.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’åˆ†æã—ã¦é©åˆ‡ãªè³ªå•ã¨å›ç­”ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚è¨˜äº‹ã®å†…å®¹ã«åŸºã¥ã„ã¦å…·ä½“çš„ã§æœ‰ç”¨ãªè³ªå•ã¨å›ç­”ã‚’æ—¥æœ¬èªã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=1000,
                temperature=0.7
            )
            
            # Parse LLM response
            llm_response = response.choices[0].message.content.strip()
            print(f"      ğŸ“ LLM Response: {llm_response[:100]}...")
            
            try:
                # Try to parse JSON response
                qa_data = json.loads(llm_response)
                return {
                    'query1': qa_data.get('query1', ''),
                    'answer1': qa_data.get('answer1', ''),
                    'query2': qa_data.get('query2', ''),
                    'answer2': qa_data.get('answer2', '')
                }
            except json.JSONDecodeError as e:
                print(f"      âš ï¸  LLM response parsing failed: {e}")
                print(f"      Raw response: {llm_response}")
                return {
                    'query1': '',
                    'answer1': '',
                    'query2': '',
                    'answer2': ''
                }
                
        except Exception as e:
            print(f"      âŒ LLM error: {e}")
            return {
                'query1': '',
                'answer1': '',
                'query2': '',
                'answer2': ''
            }

    def extract_news_content(self, news_url: str) -> str:
        """
        Extract full content from a news article URL
        ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹URLã‹ã‚‰å®Œå…¨ãªå†…å®¹ã‚’æŠ½å‡º
        """
        try:
            print(f"      Extracting content from: {news_url}")
            response = requests.get(news_url, headers=self.headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # Try multiple content selectors for Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹
            # Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ç”¨ã®è¤‡æ•°ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚»ãƒ¬ã‚¯ã‚¿ãƒ¼ã‚’è©¦ã™
            content_selectors = [
                'div.article_body',
                'div.article-body',
                'div[class*="article"]',
                'div[class*="content"]',
                'div[class*="body"]',
                'article',
                'div.news-content',
                'div.news-body'
            ]
            
            content_text = ""
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # Remove script and style elements
                    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¨ã‚¹ã‚¿ã‚¤ãƒ«è¦ç´ ã‚’å‰Šé™¤
                    for script in content_elem(["script", "style"]):
                        script.decompose()
                    
                    # Get text content
                    # ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
                    content_text = content_elem.get_text(separator='\n', strip=True)
                    if content_text and len(content_text) > 50:
                        print(f"      Found content using selector: {selector}")
                        break
            
            if not content_text:
                # Fallback: try to get any text content
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ†ã‚­ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’å–å¾—
                content_text = soup.get_text(separator='\n', strip=True)
                # Limit content length to avoid too long text
                # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®é•·ã•ã‚’åˆ¶é™ã—ã¦é•·ã™ãã‚‹ãƒ†ã‚­ã‚¹ãƒˆã‚’é¿ã‘ã‚‹
                if len(content_text) > 1000:
                    content_text = content_text[:1000] + "..."
            
            return content_text
            
        except Exception as e:
            print(f"      Error extracting content: {e}")
            return "å†…å®¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸã€‚"

    def fetch_category_news(self, category_name: str, url: str, top_n: int = 2) -> List[Dict]:
        """
        Fetch news from a specific category page with content extraction and LLM analysis
        ç‰¹å®šã®ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—ã—ã€å†…å®¹ã‚‚æŠ½å‡ºã—ã€LLMã§åˆ†æ
        """
        try:
            print(f"    Fetching: {url}")
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            results = []
            
            # Look for news links with /articles/ in href
            # hrefã«/articles/ã‚’å«ã‚€ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªãƒ³ã‚¯ã‚’æ¢ã™
            article_links = soup.find_all('a', href=True)
            news_links = [link for link in article_links if '/articles/' in link.get('href', '')]
            
            print(f"    Found {len(news_links)} article links")
            
            # Process first top_n news links
            # æœ€åˆã®top_nå€‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒªãƒ³ã‚¯ã‚’å‡¦ç†
            for i, link in enumerate(news_links[:top_n]):
                try:
                    href = link.get('href', '')
                    title = link.get_text(strip=True)
                    
                    # Clean title
                    # ã‚¿ã‚¤ãƒˆãƒ«ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                    if title and len(title) > 5 and not title.startswith('http'):
                        # Build full URL
                        # å®Œå…¨ãªURLã‚’æ§‹ç¯‰
                        if href.startswith('/'):
                            full_url = 'https://news.yahoo.co.jp' + href
                        else:
                            full_url = href
                        
                        # Extract full content
                        # å®Œå…¨ãªå†…å®¹ã‚’æŠ½å‡º
                        content = self.extract_news_content(full_url)
                        
                        # Generate queries and answers using LLM
                        # LLMã‚’ä½¿ç”¨ã—ã¦ã‚¯ã‚¨ãƒªã¨å›ç­”ã‚’ç”Ÿæˆ
                        qa_data = self.generate_queries_and_answers(content, category_name)
                        
                        # Only create entries if we have valid Q&A from LLM
                        if qa_data['query1'] and qa_data['answer1']:
                            # Create first entry with query1
                            # query1ã§æœ€åˆã®ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ
                            results.append({
                                'data_id': f"{category_name}_{i+1}_1",
                                'category': category_name,
                                'title': title,
                                'link': full_url,
                                'content': content,
                                'query': qa_data['query1'],
                                'answer': qa_data['answer1'],
                                'timestamp': datetime.now().isoformat(),
                                'source': 'Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹',
                                'crawler_version': 'unified_v1.0'
                            })
                        
                        if qa_data['query2'] and qa_data['answer2']:
                            # Create second entry with query2  
                            # query2ã§2ç•ªç›®ã®ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ
                            results.append({
                                'data_id': f"{category_name}_{i+1}_2",
                                'category': category_name,
                                'title': title,
                                'link': full_url,
                                'content': content,
                                'query': qa_data['query2'],
                                'answer': qa_data['answer2'],
                                'timestamp': datetime.now().isoformat(),
                                'source': 'Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹',
                                'crawler_version': 'unified_v1.0'
                            })
                        
                        # Add delay between content extraction and LLM analysis requests
                        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æŠ½å‡ºã¨LLMåˆ†æãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«é…å»¶ã‚’è¿½åŠ 
                        time.sleep(2)
                        
                except Exception as e:
                    print(f"    Error processing link {i+1}: {e}")
                    continue
            
            return results
            
        except Exception as e:
            print(f"    Error fetching {category_name}: {e}")
            return []

    def save_to_json(self, data: List[Dict], filename: str = None) -> str:
        """
        Save news data to JSON file
        ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’JSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        """
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"yahoo_news_dataset_{timestamp}.json"
        
        filepath = os.path.join(self.output_dir, filename)
        
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return filepath

    def crawl_all_categories(self) -> List[Dict]:
        """
        Crawl all category pages and generate Q&A dataset
        å…¨ã‚«ãƒ†ã‚´ãƒªãƒšãƒ¼ã‚¸ã‚’ã‚¯ãƒ­ãƒ¼ãƒ«ã—ã¦Q&Aãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ç”Ÿæˆ
        """
        print("ğŸš€ Starting Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ Universal Crawler...")
        print("=" * 80)
        
        # Check OpenAI API key
        if not os.getenv('OPENAI_API_KEY'):
            print("âŒ OpenAI API key not found. Cannot generate Q&A without LLM.")
            print("   Set OPENAI_API_KEY environment variable to proceed.")
            print("   Exiting...")
            return []
        
        all_news = []
        
        for category, url in self.category_urls.items():
            print(f"ğŸ”„ Crawling category: {category}")
            try:
                news = self.fetch_category_news(category, url, top_n=2)
                all_news.extend(news)
                print(f"   âœ… Found {len(news)} entries with LLM-generated Q&A")
                
                # Add delay between category requests
                # ã‚«ãƒ†ã‚´ãƒªãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“ã«é…å»¶ã‚’è¿½åŠ 
                time.sleep(3)
                
            except Exception as e:
                print(f"   âŒ Error: {e}")
        
        return all_news

    def run(self, save_results: bool = True) -> Optional[str]:
        """
        Run the complete crawler pipeline
        å®Œå…¨ãªã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
        """
        # Crawl all categories
        all_news = self.crawl_all_categories()
        
        if not all_news:
            print("âš ï¸  No news data collected")
            return None
        
        print(f"\nğŸ“Š Total entries collected: {len(all_news)}")
        
        if save_results:
            # Save to JSON file
            filepath = self.save_to_json(all_news)
            print(f"ğŸ’¾ News data saved to: {filepath}")
            
            # Display statistics
            print(f"\nğŸ“ˆ Statistics:")
            print(f"   Total entries: {len(all_news)}")
            print(f"   Categories found: {len(set(n['category'] for n in all_news))}")
            
            # Show category breakdown
            categories = {}
            for news in all_news:
                cat = news['category']
                categories[cat] = categories.get(cat, 0) + 1
            
            for cat, count in sorted(categories.items()):
                print(f"   - {cat}: {count} entries")
            
            # Content statistics
            content_lengths = [len(news.get('content', '')) for news in all_news]
            if content_lengths:
                avg_length = sum(content_lengths) / len(content_lengths)
                print(f"   Average content length: {avg_length:.0f} characters")
            
            # Q&A statistics
            qa_count = sum(1 for news in all_news if news.get('query') and news.get('answer'))
            print(f"   Entries with Q&A generated: {qa_count}/{len(all_news)}")
            
            return filepath
        
        return all_news


def main():
    """
    Main function to run the crawler
    ã‚¯ãƒ­ãƒ¼ãƒ©ãƒ¼ã‚’å®Ÿè¡Œã™ã‚‹ãƒ¡ã‚¤ãƒ³é–¢æ•°
    """
    crawler = YahooNewsCrawler()
    result = crawler.run()
    
    if result:
        print(f"\nâœ… Success! Dataset saved to: {result}")
    else:
        print("\nâŒ Failed to generate dataset")


if __name__ == "__main__":
    main()