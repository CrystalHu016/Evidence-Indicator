#!/usr/bin/env python3
"""
Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹ Category News Crawler with LLM Integration
çˆ¬å–æ¯ä¸ªç±»åˆ«ä¸‹çš„æœ€æ–°æ–°é—»å¹¶ç”Ÿæˆæ—¥è¯­æŸ¥è¯¢å’Œç­”æ¡ˆ
"""

import requests
from bs4 import BeautifulSoup
import os
import json
import time
import openai
from typing import List, Dict, Optional, Tuple
from datetime import datetime
import logging
from dotenv import load_dotenv
import re

# Load environment variables
load_dotenv()

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class CategoryNewsCrawler:
    """Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†ç±»æ–°é—»çˆ¬è™«ä¸LLMé›†æˆç³»ç»Ÿ"""
    
    def __init__(self, openai_api_key: str = None):
        self.base_url = 'https://news.yahoo.co.jp'
        
        # å„ä¸ªåˆ†ç±»çš„URL
        self.category_urls = {
            'ä¸»è¦': 'https://news.yahoo.co.jp',
            'å›½å†…': 'https://news.yahoo.co.jp/categories/domestic',
            'å›½éš›': 'https://news.yahoo.co.jp/categories/international',
            'çµŒæ¸ˆ': 'https://news.yahoo.co.jp/categories/economy',
            'ã‚¨ãƒ³ã‚¿ãƒ¡': 'https://news.yahoo.co.jp/categories/entertainment',
            'ã‚¹ãƒãƒ¼ãƒ„': 'https://news.yahoo.co.jp/categories/sports',
            'IT': 'https://news.yahoo.co.jp/categories/it',
            'ç§‘å­¦': 'https://news.yahoo.co.jp/categories/science',
            'ãƒ©ã‚¤ãƒ•': 'https://news.yahoo.co.jp/categories/life',
            'åœ°åŸŸ': 'https://news.yahoo.co.jp/categories/local'
        }
        
        # OpenAIé…ç½®
        self.openai_api_key = openai_api_key or os.getenv('OPENAI_API_KEY')
        if self.openai_api_key:
            openai.api_key = self.openai_api_key
            logger.info("âœ… OpenAI API key loaded")
        else:
            logger.warning("âš ï¸ OpenAI API key not found")
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        self.output_dir = 'yahoo_news_dataset'
        os.makedirs(self.output_dir, exist_ok=True)
        
        # è¯·æ±‚å¤´è®¾ç½®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        }
    
    def get_page_content(self, url: str, delay: float = 2.0) -> Optional[str]:
        """è·å–ç½‘é¡µå†…å®¹ï¼ŒåŒ…å«å»¶è¿Ÿå’Œé”™è¯¯å¤„ç†"""
        try:
            logger.info(f"ğŸ”„ Fetching: {url}")
            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¢«åçˆ¬è™«
            time.sleep(delay)
            return response.text
            
        except requests.exceptions.RequestException as e:
            logger.error(f"âŒ Error fetching {url}: {e}")
            return None
        except Exception as e:
            logger.error(f"âŒ Unexpected error: {e}")
            return None
    
    def extract_news_from_category(self, html_content: str, category: str) -> List[Dict]:
        """ä»åˆ†ç±»é¡µé¢æå–æ–°é—»ä¿¡æ¯"""
        news_list = []
        soup = BeautifulSoup(html_content, 'html.parser')
        
        try:
            # å°è¯•å¤šç§é€‰æ‹©å™¨æ¥æ‰¾åˆ°æ–°é—»é“¾æ¥
            news_selectors = [
                'a[href*="/articles/"]',
                'a[href*="/topics/"]',
                '.topicsList a',
                '.newsFeed a',
                '.newsList a'
            ]
            
            news_links = []
            for selector in news_selectors:
                links = soup.select(selector)
                if links:
                    news_links = links
                    break
            
            # æå–æ–°é—»ä¿¡æ¯
            for link in news_links[:10]:  # è·å–å‰10ä¸ªæ–°é—»
                href = link.get('href')
                if href and '/articles/' in href:
                    # æ„å»ºå®Œæ•´URL
                    if href.startswith('/'):
                        full_url = self.base_url + href
                    else:
                        full_url = href
                    
                    # æå–æ ‡é¢˜
                    title = link.get_text(strip=True)
                    if title and len(title) > 10:  # è¿‡æ»¤æ‰å¤ªçŸ­çš„æ ‡é¢˜
                        news_list.append({
                            'title': title,
                            'url': full_url,
                            'category': category
                        })
            
            logger.info(f"ğŸ“° Found {len(news_list)} news articles in {category}")
            
        except Exception as e:
            logger.error(f"âŒ Error extracting news from {category}: {e}")
        
        return news_list
    
    def get_news_content(self, news_url: str) -> Optional[Dict]:
        """è·å–å•ä¸ªæ–°é—»çš„è¯¦ç»†å†…å®¹"""
        try:
            html_content = self.get_page_content(news_url, delay=1.0)
            if not html_content:
                return None
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–æ–°é—»å†…å®¹
            content = ""
            
            # å°è¯•å¤šç§é€‰æ‹©å™¨
            content_selectors = [
                '.article_body',
                '.articleBody',
                '.newsContent',
                '.content',
                'article p',
                '.article p'
            ]
            
            for selector in content_selectors:
                elements = soup.select(selector)
                if elements:
                    content = ' '.join([elem.get_text(strip=True) for elem in elements])
                    break
            
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å†…å®¹ï¼Œå°è¯•æå–æ ‡é¢˜
            if not content:
                title_elem = soup.find('h1') or soup.find('title')
                if title_elem:
                    content = title_elem.get_text(strip=True)
            
            if content:
                return {
                    'content': content,
                    'url': news_url
                }
            
        except Exception as e:
            logger.error(f"âŒ Error getting news content from {news_url}: {e}")
        
        return None
    
    def generate_query_and_answer(self, news_content: str, category: str) -> Tuple[str, str]:
        """ä½¿ç”¨LLMç”ŸæˆæŸ¥è¯¢å’Œç­”æ¡ˆ"""
        try:
            if not self.openai_api_key:
                # å¦‚æœæ²¡æœ‰API keyï¼Œç”Ÿæˆç®€å•çš„æŸ¥è¯¢å’Œç­”æ¡ˆ
                return self.generate_simple_qa(news_content, category)
            
            # ä½¿ç”¨OpenAI APIç”ŸæˆæŸ¥è¯¢å’Œç­”æ¡ˆ
            prompt = f"""
ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®å†…å®¹ã«åŸºã¥ã„ã¦ã€æ—¥æœ¬èªã§è³ªå•ã¨å›ç­”ã‚’ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

è¨˜äº‹å†…å®¹: {news_content[:1000]}
ã‚«ãƒ†ã‚´ãƒª: {category}

è¦æ±‚:
1. è³ªå•ã¯è¨˜äº‹ã®å†…å®¹ã«ç›´æ¥ç­”ãˆã‚‰ã‚Œã‚‹ã‚‚ã®ã«ã—ã¦ãã ã•ã„
2. è³ªå•ã¨å›ç­”ã¯ä¸¡æ–¹ã¨ã‚‚æ—¥æœ¬èªã§æ›¸ã„ã¦ãã ã•ã„
3. è³ªå•ã¯è‡ªç„¶ã§è‡ªç„¶ãªæ—¥æœ¬èªè¡¨ç¾ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„
4. å›ç­”ã¯è¨˜äº‹ã®å†…å®¹ã«åŸºã¥ã„ã¦æ­£ç¢ºã§ç°¡æ½”ã«ã—ã¦ãã ã•ã„

å‡ºåŠ›å½¢å¼:
è³ªå•: [è³ªå•å†…å®¹]
å›ç­”: [å›ç­”å†…å®¹]
"""
            
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[
                    {"role": "system", "content": "ã‚ãªãŸã¯ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‹ã‚‰è³ªå•ã¨å›ç­”ã‚’ç”Ÿæˆã™ã‚‹å°‚é–€å®¶ã§ã™ã€‚"},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=500,
                temperature=0.7
            )
            
            result = response.choices[0].message.content.strip()
            
            # è§£æç»“æœ
            lines = result.split('\n')
            query = ""
            answer = ""
            
            for line in lines:
                if line.startswith('è³ªå•:'):
                    query = line.replace('è³ªå•:', '').strip()
                elif line.startswith('å›ç­”:'):
                    answer = line.replace('å›ç­”:', '').strip()
            
            if query and answer:
                return query, answer
            else:
                return self.generate_simple_qa(news_content, category)
                
        except Exception as e:
            logger.error(f"âŒ Error generating query and answer: {e}")
            return self.generate_simple_qa(news_content, category)
    
    def generate_simple_qa(self, news_content: str, category: str) -> Tuple[str, str]:
        """ç”Ÿæˆç®€å•çš„æŸ¥è¯¢å’Œç­”æ¡ˆï¼ˆå¤‡ç”¨æ–¹æ³•ï¼‰"""
        # ä»å†…å®¹ä¸­æå–å…³é”®ä¿¡æ¯
        content_preview = news_content[:100] + "..." if len(news_content) > 100 else news_content
        
        # æ ¹æ®ç±»åˆ«ç”Ÿæˆä¸åŒç±»å‹çš„æŸ¥è¯¢
        query_templates = {
            'ä¸»è¦': f"ã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ä¸»ãªå†…å®¹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            'å›½å†…': f"ã“ã®å›½å†…ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è©³ç´°ã¯ã©ã®ã‚ˆã†ãªã‚‚ã®ã§ã™ã‹ï¼Ÿ",
            'å›½éš›': f"ã“ã®å›½éš›ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è¦ç‚¹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            'çµŒæ¸ˆ': f"ã“ã®çµŒæ¸ˆãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å½±éŸ¿ã¯ã©ã®ã‚ˆã†ãªã‚‚ã®ã§ã™ã‹ï¼Ÿ",
            'ã‚¨ãƒ³ã‚¿ãƒ¡': f"ã“ã®ã‚¨ãƒ³ã‚¿ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®å†…å®¹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            'ã‚¹ãƒãƒ¼ãƒ„': f"ã“ã®ã‚¹ãƒãƒ¼ãƒ„ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®çµæœã¯ã©ã†ã§ã—ãŸã‹ï¼Ÿ",
            'IT': f"ã“ã®ITãƒ‹ãƒ¥ãƒ¼ã‚¹ã®æŠ€è¡“çš„ãªå†…å®¹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            'ç§‘å­¦': f"ã“ã®ç§‘å­¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ç™ºè¦‹ã‚„ç ”ç©¶å†…å®¹ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            'ãƒ©ã‚¤ãƒ•': f"ã“ã®ãƒ©ã‚¤ãƒ•ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®ç”Ÿæ´»ã¸ã®å½±éŸ¿ã¯ä½•ã§ã™ã‹ï¼Ÿ",
            'åœ°åŸŸ': f"ã“ã®åœ°åŸŸãƒ‹ãƒ¥ãƒ¼ã‚¹ã®è©³ç´°ã¯ã©ã®ã‚ˆã†ãªã‚‚ã®ã§ã™ã‹ï¼Ÿ"
        }
        
        query = query_templates.get(category, "ã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®å†…å®¹ã¯ä½•ã§ã™ã‹ï¼Ÿ")
        answer = f"ã“ã®è¨˜äº‹ã¯{category}ã‚«ãƒ†ã‚´ãƒªã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã€{content_preview}ã«ã¤ã„ã¦å ±ã˜ã¦ã„ã¾ã™ã€‚"
        
        return query, answer
    
    def crawl_category_news(self) -> List[Dict]:
        """çˆ¬å–æ‰€æœ‰åˆ†ç±»çš„æœ€æ–°æ–°é—»"""
        all_news_data = []
        
        for category, url in self.category_urls.items():
            logger.info(f"ğŸ”„ Crawling category: {category}")
            
            try:
                # è·å–åˆ†ç±»é¡µé¢å†…å®¹
                html_content = self.get_page_content(url, delay=2.0)
                if not html_content:
                    logger.warning(f"âš ï¸ Failed to get content for {category}")
                    continue
                
                # æå–æ–°é—»åˆ—è¡¨
                news_list = self.extract_news_from_category(html_content, category)
                
                # è·å–å‰2ä¸ªæ–°é—»çš„è¯¦ç»†å†…å®¹
                for i, news in enumerate(news_list[:2]):
                    logger.info(f"ğŸ“° Processing news {i+1} in {category}: {news['title'][:50]}...")
                    
                    # è·å–æ–°é—»è¯¦ç»†å†…å®¹
                    news_detail = self.get_news_content(news['url'])
                    if news_detail:
                        # ç”ŸæˆæŸ¥è¯¢å’Œç­”æ¡ˆ
                        query, answer = self.generate_query_and_answer(
                            news_detail['content'], 
                            category
                        )
                        
                        # åˆ›å»ºæ•°æ®æ¡ç›®
                        news_data = {
                            "core_query": query,
                            "answer": answer,
                            "original_urls": [news['url']],
                            "content_summary": news['title'],
                            "timestamp": datetime.now().isoformat(),
                            "source": "Yahoo!ãƒ‹ãƒ¥ãƒ¼ã‚¹",
                            "category": category,
                            "crawler_version": "category_news_v1.0"
                        }
                        
                        all_news_data.append(news_data)
                        logger.info(f"âœ… Generated Q&A for {category} news {i+1}")
                    
                    # æ·»åŠ å»¶è¿Ÿ
                    time.sleep(1.0)
                
            except Exception as e:
                logger.error(f"âŒ Error crawling category {category}: {e}")
                continue
        
        return all_news_data
    
    def save_dataset(self, news_data: List[Dict], filename: str = None):
        """ä¿å­˜æ•°æ®é›†åˆ°æ–‡ä»¶"""
        if not filename:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"category_news_dataset_{timestamp}.json"
        
        filepath = os.path.join(self.output_dir, filename)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(news_data, f, ensure_ascii=False, indent=2)
            
            logger.info(f"ğŸ’¾ Dataset saved to: {filepath}")
            logger.info(f"ğŸ“Š Total entries: {len(news_data)}")
            
        except Exception as e:
            logger.error(f"âŒ Error saving dataset: {e}")
    
    def run(self):
        """è¿è¡Œçˆ¬è™«"""
        logger.info("ğŸš€ Starting Category News Crawler...")
        
        try:
            # çˆ¬å–æ‰€æœ‰åˆ†ç±»çš„æ–°é—»
            news_data = self.crawl_category_news()
            
            if news_data:
                # ä¿å­˜æ•°æ®é›†
                self.save_dataset(news_data)
                logger.info("âœ… Crawling completed successfully!")
            else:
                logger.warning("âš ï¸ No news data collected")
                
        except Exception as e:
            logger.error(f"âŒ Crawler failed: {e}")

def main():
    """ä¸»å‡½æ•°"""
    # åˆ›å»ºçˆ¬è™«å®ä¾‹
    crawler = CategoryNewsCrawler()
    
    # è¿è¡Œçˆ¬è™«
    crawler.run()

if __name__ == "__main__":
    main()
